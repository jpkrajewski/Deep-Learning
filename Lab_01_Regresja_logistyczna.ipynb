{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "\n",
    "<font size=\"5\">\n",
    "\n",
    "Laboratorium z przedmiotu: \\\n",
    "**Głębokie uczenie i analiza obrazów**\n",
    "\n",
    "Ćwiczenie 1: \\\n",
    "**Regresja logistyczna i inne klasyczne algorytmy klasyfikacji**\n",
    "\n",
    "</font>\n",
    "\n",
    "\\\n",
    "Marta Szarmach \\\n",
    "Zakład Telekomunikacji Morskiej \\\n",
    "Wydział Elektryczny \\\n",
    "Uniwersytet Morski w Gdyni\n",
    "\n",
    "07.2023\n",
    "</div>\n",
    "\n",
    "\n",
    "# 1. Wprowadzenie\n",
    "\n",
    "**Uczenie maszynowe** jest to zbiór algorytmów, które odnajdują zależności ukryte w danych, potrafią te zależności modelować (tj. opisywać za pomocą matematycznych struktur) bez bycia jawnie zaprogramowanym przez człowieka i doskonalą swoje działanie (tj. potrafią dobierać lepsze parametry opisujące modele) dzięki dostarczeniu do nich nowych danych (uczenie maszynowe jest jedynie częścią szerszego zagadnienia, jakim jest **sztuczna inteligencja**, tj. dziedzina nauki z pogranicza informatyki, matematyki, kongwinistyki i neurologii, zajmująca się tworzeniem maszyn/oprogramowania ,,udających'' ludzką inteligencję, tj. potrafiącego analizować dostarczone doń dane, wyciągać na ich podstawie wnioski i podejmować decyzje).  Innymi słowy, w przeciwieństwie do klasycznego programowania, nie tworzy się gotowych reguł, lecz algorytm sam, na podstawie dostarczonych danych i spodziewanych odpowiedzi na nie, określa reguły, tzn. tworzy **model**, który stara się odzwierciedlić strukturę danych i sposób wyznaczania tychże odpowiedzi. Model najczęściej opisany jest za pomocą fukcji (hipotezy) zależnej od **parametrów** $\\theta$, które są pewnymi wartościami liczbowymi ulegającymi zmianie w ramach treningu (nie należy mylić parametrów z **hiperparametrami**, które również opisują w pewnym sensie model, lecz nie są wyznaczane podczas treningu, a wręcz przeciwnie, muszą być podane wcześniej).\n",
    "\n",
    "Aby stworzyć i wytrenować model, najczęściej określa się pewną funkcję (nazywaną **funkcją kosztu** $J(\\theta)$), która jest w stanie policzyć, jak bardzo model myli się podczas dokonywania predykcji. **Trening** polega na optymalizacji funkcji kosztu, a dokładniej mówiąc, iteracyjnej aktualizacji parametrów modelu, aż koszt na danych treningowych staje się (najczęściej) minimalny - wówczas uważa się, że model możliwie najlepiej odzwierciedla te dane. Jedną z metod optymalizacji jest **metoda gradientu prostego**, której działanie opiera się na poszukiwaniu lokalnego minimum poprzez wyznaczanie gradientu $\\frac{\\partial J(\\theta)}{\\partial \\theta}$ (kierunku najszybszej zmiany) funkcji kosztu i aktualizacji parametrów zgodnie z tym gradientem. \n",
    "\n",
    "\\\n",
    "Uczenie maszynowe można podzielić na dwie podstawowe grupy: \n",
    "* **uczenie nadzorowane**, w którym każdy rekord danych dostarczony do algorytmu posiada **etykietę** zawierającą pożądaną odpowiedź algorytmu na ten konkretny rekord. Algorytmy uczenia nadzorowanego potrafią rozwiązywać takie problemy jak:\n",
    "    * **regresja** - przypisanie do rekordu danych pewnej dowolnej liczby,\n",
    "    * **klasyfikacja** -  przypisanie do rekordu danych liczby (z przedziału dyskretnego), symbolizującej jego przynależność do pewnej klasy,\n",
    "* **uczenie nienadzorowane** - dane nie posiadają predefiniowanych etykiet, algorytmy muszą same znaleźć strukturę w danych. Rozwiązują takie problemy, jak m.in.:\n",
    "    * **grupowanie** - podział zebranych danych na grupy, tak, aby dane z jednej grupy były bardziej podobne do siebie niż do danych z innych grup,\n",
    "    * **wykrywanie anomalii** - odnalezienie w zbiorze danych tych rekordów, które w pewien sposób odróżniają się od reszty.\n",
    "\n",
    "W niniejszym ćwiczeniu zajmować się będziemy jedynie zagadnieniem klasyfikacji. Niektórymi algorytmami uczenia maszynowego realizującymi klasyfikację są:\n",
    "* **Maszyna wektrów wspierających** (ang. *Support Vector Machine*, **SVM**) - algorytm, który dokonuje klasyfikacji danych poprzez utworzenie (z pomocą dodatkowych funkcji, tzw. kerneli) dodatkowego wymiaru i hiperpłaszczyzny, która oddziela na tym wymiarze dane z różnych klas z maksymalnym możliwym marginesem,\n",
    "* **Drzewo decyzyjne** (ang. *Decision Tree*) - zbiór hierarchicznie następujących po sobie instrukcji warunkowych, których ostatnia warstwa decyduje o wyniku predykcji,\n",
    "* *k* **najbliższych sąsiadów** (ang. *k Nearest Neighbours*, *k*-NN) - to, jaka zostanie podjęta decyzja dotycząca badanego rekordu, zależy od etykiet $k$ innych rekordów najbliższych temu rekordowi,\n",
    "* a także **regresja logistyczna** - która to będzie głównym zagadnieniem niniejszego ćwiczenia. W toku działania tegoż algorytmu, dokonuje się dopasowania pewnej funkcji, wiążącej parametry $\\theta$ i dane X, której zadaniem jest oszacowanie prawdopodobieństwa, z jakim określony rekord danych należy do pewnej klasy.\n",
    "\n",
    "<div align=\"center\">\n",
    "\n",
    "<img src='https://miro.medium.com/v2/resize:fit:828/1*PQ8tdohapfm-YHlrRIRuOA.gif' />\n",
    "\n",
    "<font size=\"1\">Grafika: towardsdatascience.com</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Cel ćwiczenia\n",
    "\n",
    "**Celem niniejszego ćwiczenia** jest zapoznanie się z działaniem klasycznych algorytmów uczenia maszynowego realizujących zagadnienia klasyfikacji poprzez:\n",
    "* implementacji ,,od zera'' algorytmu regresji logistycznej (łącznie z optymalizacją funkcji kosztu metodą gradientu prostego),\n",
    "* użycie gotowych klas z biblioteki Scikit-learn z zaimplementowanymi gotowymi klasyfikatorami (regresją logistyczną, drzewem decyzyjnym i *k*-NN) i porównanie otrzymanych wyników.\n",
    "\n",
    "\n",
    "# 3. Stanowisko laboratoryjne\n",
    "\n",
    "Do wykonania niniejszego ćwiczenia niezbędne jest stanowisko laboratoryjne, składające się z komputera klasy PC z zainstalowanym oprogramowaniem:\n",
    "* językiem programowania Python (w wersji 3.8),\n",
    "* IDE obsługującym pliki Jupyter Notebook (np. Visual Studio Code z rozszerzeniem ipykernel).\n",
    "\n",
    "\n",
    "# 4. Przebieg ćwiczenia\n",
    "## 4.1. Implementacja algorytmu regresji logistycznej ,,od zera''\n",
    "\n",
    "### Inicjalizacja: import niezbędnych elementów\n",
    "\n",
    "Na początku wykonaj poniższy fragment kodu, aby zaimportować biblioteki niezbędne do wykonania poniższego ćwiczenia:\n",
    "* **Scikit-learn** - biblioteka zawierająca gotowe implementacje wielu algorytmów klasycznego uczenia maszynowego, a także zbiory danych czy metryki. Tutaj skorzystamy ze zbioru danych iris - `datasets.load_iris`.\n",
    "* **NumPy** - biblioteka umożliwiająca wykonywanie wysoko zoptymalizowanych obliczeń matematycznych na objektach typu *numpy array* (wielowymiarowych tablic).\n",
    "* **Matplotlib** - biblioteka wspomagająca wizualizację pracy czy analizę danych poprzez wyświetlanie wykresów."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: line 1: python: command not found\n",
      "/bin/bash: line 1: python: command not found\n",
      "/bin/bash: line 1: python: command not found\n"
     ]
    }
   ],
   "source": [
    "! python -m pip install numpy==1.22.3\n",
    "! python -m pip install scikit-learn==0.24.2\n",
    "! python -m pip install matplotlib==3.4.2\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Po zaimportowaniu niezbędnych bibliotek, załadujmy gotowy zbiór danych Iris (zawierający dane o wymiarach różnych rodzajów irysów), pochodzący z repozytorium UCI (więcej informacji o tym zbiorze danych możesz uzyskać [TUTAJ](https://archive.ics.uci.edu/dataset/53/iris)). Dane zapiszmy pod zmienną X, a odpowiadające im etykiety - y.\n",
    "\n",
    "Ponadto, dla zobrazowania danych, które będziemy używać, wyświetlmy rozmiary tablic X i y, a także 5 pierszwych rekordów/etykiet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wymiary danych wejściowych: (100, 4)\n",
      "Przykładowe dane wejściowe: \n",
      "[[5.1 3.5 1.4 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.  3.6 1.4 0.2]]\n",
      "Wymiary etykiet: (100,)\n",
      "Przykładowe etykiety: \n",
      "[0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "iris = load_iris()\n",
    "X = iris.data[0:100,:]\n",
    "y = iris.target[0:100]\n",
    "print(\"Wymiary danych wejściowych: \" + str(X.shape))\n",
    "print(\"Przykładowe dane wejściowe: \")\n",
    "print(X[0:5,:])\n",
    "print(\"Wymiary etykiet: \" + str(y.shape))\n",
    "print(\"Przykładowe etykiety: \")\n",
    "print(y[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['setosa' 'versicolor' 'virginica']\n"
     ]
    }
   ],
   "source": [
    "iris2 = load_iris()\n",
    "print(iris2['target_names'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Przyjrzyjmy się wyświetlonym informacjom na temat wczytanych danych. Wczytujemy 100 rekordów o 4 cechach (wymiary tablicy X to 100x4). Etykiety zapisane są w formie 1-wymiarowego wektora y (o wymiarze 100) - z dokumentacji możemy się dowiedzieć, że mogą one przyjmować jedną z 3 wartości: 0, 1 lub 2, symbolizujących przynależność rekordu do jednej z 3 klas (każda klasa oznacza inny typ irysa), lecz w naszym przypadku, aby skupić się jedynie na zagadnieniu binarnej klasyfikacji, podczas ładowania danych odrzucamy ostatnich 50 rekordów należących do 3 klasy, więc operujemy jedynie na 2 klasach.\n",
    "\n",
    "<font size=\"2\">Informacje o wymiarach otrzymanych tablic będą nam bardzo potrzebne na późniejszym etapie, podczas pracy na macierzach przy implementacji procesu terningu i predykcji naszego algorytmu.</font>\n",
    "\n",
    "\n",
    "### Przygotowanie danych\n",
    "\n",
    "Aby móc w pełni korzystać z tych danych, musimy je jednak nieco przekształcić. W tym celu wykonamy dwie operacje:\n",
    "* dokonamy **standaryzacji** danych, tj. przekształcimy je tak, aby bez zmiany ich struktury, każda z cech posiadała średnią o wartości 0 i wariancję o wartości 1 - poprawi to działanie algorytmów uczenia maszynowego, zwłaszcza podczas pracy nad danymi o szerokiej dynamice (różnych rzędach wielkości) czy różnych jednostkach,\n",
    "* podzielimy cały dostępny zbiór danych na 2 zestawy: \n",
    "    * **treningowy** - na podstawie którego model zostanie wytrenowany (to pod te dane zostaną dopasowane parametry naszego modelu),\n",
    "    * **testowy** - który posłuży nam do określenia, jak dobrze działa nasz model na danych, których wcześniej model nie widział.\n",
    "\n",
    "<font size=\"2\">W tym konkretnym przypadku nie będziemy jeszcze wydzielać trzeciego z zazwyczaj tworzonych zestawów danych, zestawu **walidacyjnego**, na podstawie którego dopasowuje się hiperparametry modelu zanim przejdzie się do jego właściwego uczenia - nie ma takiej potrzeby, gdyż nie będziemy w ramach tego ćwiczenia zajmować się dopasowaniem żadnych hiperparametrów.</font>\n",
    "\n",
    "Zacznijmy od napisania funkcji `standarize_data` służącej do normalizacji danych. Zgodnie z poniższym wzorem, zaimplementuj funkcję, która przyjmuje jako argument wejściowe (tablicę X), oblicza średnią $\\mu_n$ i odchylenie standardowe $\\sigma_n$ każdej z cech $x_n$, a następnie odejmuje od nich wyliczone średnie i dzieli je przez odchylenia stardardowe: \n",
    "\\begin{equation*}\n",
    "\tx_n = \\frac{x_n - \\mu_n}{\\sigma_n}\n",
    "\\end{equation*}\n",
    "po czym zwraca tak znormalizowane dane jako tablicę Xnorm.\n",
    "\n",
    "<font size=\"2\">Wskazówki przydatne przy implementacji funkcji `standarize_data`:\n",
    "* Zapoznaj się z dokumentacją dwóch funkcji z biblioteki NumPy: `np.mean` ([TUTAJ](https://numpy.org/doc/stable/reference/generated/numpy.mean.html)) oraz `np.std` ([TUTAJ](https://numpy.org/doc/stable/reference/generated/numpy.std.html)). Zwróć uwagę na parametr `axis`.\n",
    "* Zastosuj mechanizm broadcastigu występujący podczas operacji na tablicach z użyciem NumPy (więcej o tym możesz przeczytać [TUTAJ](https://numpy.org/doc/stable/user/basics.broadcasting.html)).\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.63848179, 0.47633916, 1.44228257, 0.56232019]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.average(X, axis=0, keepdims=True)\n",
    "np.std(X, axis=0, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standarize_data(X):\n",
    "    \"\"\"Funkcja realizująca standaryzację danych: przyrównanie średniej każdej z cech do 0, \n",
    "    a wariancji do 1.\n",
    "\n",
    "    Argument: \n",
    "        - X - nieprzekształcone dane (numpy array, shape = (num_samples, num_features) ). \\n\n",
    "\n",
    "    Zwraca:\n",
    "        - Xnorm - znormalizowane dane (numpy array, shape = (num_samples, num_features) ), \\n\n",
    "        - mu - wektor ze średnimi każdej z cech (numpy array, shape = (num_features,) ), \\n\n",
    "        - sigma - wektor z odchyleniami standardowymi każdej z cech (numpy array, shape = (num_features,) ).\"\"\"\n",
    "\n",
    "    # ------- UZUPEŁNIJ KOD --------\n",
    "    # Oblicz mu - średnią każdej z cech (pamiętaj, że cechy przechowywane są w każdej kolummnie tablicy X)\n",
    "    mu = np.average(X, axis=0, keepdims=True)\n",
    "    # Oblicz sigma - odchylenie standardowe każdej z cech\n",
    "    sigma = np.std(X, axis=0, keepdims=True)\n",
    "    # Oblicz Xnorm - odejmij średnią od każdej z cech i podziel ją przez jej odchylenie standardowe \n",
    "    # (możesz to zrobić w 1 linijce kodu)\n",
    "    Xnorm = (np.array(X) - mu) / sigma\n",
    "    # -------------------------------\n",
    "    return Xnorm, mu, sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-5.81065904e-01,  8.41837140e-01, -1.01297765e+00,\n",
       "         -1.04211089e+00],\n",
       "        [-8.94308978e-01, -2.07835104e-01, -1.01297765e+00,\n",
       "         -1.04211089e+00],\n",
       "        [-1.20755205e+00,  2.12033793e-01, -1.08231219e+00,\n",
       "         -1.04211089e+00],\n",
       "        [-1.36417359e+00,  2.09934449e-03, -9.43643106e-01,\n",
       "         -1.04211089e+00],\n",
       "        [-7.37687441e-01,  1.05177159e+00, -1.01297765e+00,\n",
       "         -1.04211089e+00],\n",
       "        [-1.11201292e-01,  1.68157493e+00, -8.04974023e-01,\n",
       "         -6.86441647e-01],\n",
       "        [-1.36417359e+00,  6.31902691e-01, -1.01297765e+00,\n",
       "         -8.64276271e-01],\n",
       "        [-7.37687441e-01,  6.31902691e-01, -9.43643106e-01,\n",
       "         -1.04211089e+00],\n",
       "        [-1.67741667e+00, -4.17769553e-01, -1.01297765e+00,\n",
       "         -1.04211089e+00],\n",
       "        [-8.94308978e-01,  2.09934449e-03, -9.43643106e-01,\n",
       "         -1.21994552e+00],\n",
       "        [-1.11201292e-01,  1.26170604e+00, -9.43643106e-01,\n",
       "         -1.04211089e+00],\n",
       "        [-1.05093052e+00,  6.31902691e-01, -8.74308565e-01,\n",
       "         -1.04211089e+00],\n",
       "        [-1.05093052e+00, -2.07835104e-01, -1.01297765e+00,\n",
       "         -1.21994552e+00],\n",
       "        [-1.83403820e+00, -2.07835104e-01, -1.22098127e+00,\n",
       "         -1.21994552e+00],\n",
       "        [ 5.15284858e-01,  1.89150938e+00, -1.15164673e+00,\n",
       "         -1.04211089e+00],\n",
       "        [ 3.58663321e-01,  2.73124718e+00, -9.43643106e-01,\n",
       "         -6.86441647e-01],\n",
       "        [-1.11201292e-01,  1.68157493e+00, -1.08231219e+00,\n",
       "         -6.86441647e-01],\n",
       "        [-5.81065904e-01,  8.41837140e-01, -1.01297765e+00,\n",
       "         -8.64276271e-01],\n",
       "        [ 3.58663321e-01,  1.47164049e+00, -8.04974023e-01,\n",
       "         -8.64276271e-01],\n",
       "        [-5.81065904e-01,  1.47164049e+00, -9.43643106e-01,\n",
       "         -8.64276271e-01],\n",
       "        [-1.11201292e-01,  6.31902691e-01, -8.04974023e-01,\n",
       "         -1.04211089e+00],\n",
       "        [-5.81065904e-01,  1.26170604e+00, -9.43643106e-01,\n",
       "         -6.86441647e-01],\n",
       "        [-1.36417359e+00,  1.05177159e+00, -1.29031581e+00,\n",
       "         -1.04211089e+00],\n",
       "        [-5.81065904e-01,  4.21968242e-01, -8.04974023e-01,\n",
       "         -5.08607024e-01],\n",
       "        [-1.05093052e+00,  6.31902691e-01, -6.66304941e-01,\n",
       "         -1.04211089e+00],\n",
       "        [-7.37687441e-01, -2.07835104e-01, -8.74308565e-01,\n",
       "         -1.04211089e+00],\n",
       "        [-7.37687441e-01,  6.31902691e-01, -8.74308565e-01,\n",
       "         -6.86441647e-01],\n",
       "        [-4.24444366e-01,  8.41837140e-01, -9.43643106e-01,\n",
       "         -1.04211089e+00],\n",
       "        [-4.24444366e-01,  6.31902691e-01, -1.01297765e+00,\n",
       "         -1.04211089e+00],\n",
       "        [-1.20755205e+00,  2.12033793e-01, -8.74308565e-01,\n",
       "         -1.04211089e+00],\n",
       "        [-1.05093052e+00,  2.09934449e-03, -8.74308565e-01,\n",
       "         -1.04211089e+00],\n",
       "        [-1.11201292e-01,  6.31902691e-01, -9.43643106e-01,\n",
       "         -6.86441647e-01],\n",
       "        [-4.24444366e-01,  2.10144383e+00, -9.43643106e-01,\n",
       "         -1.21994552e+00],\n",
       "        [ 4.54202458e-02,  2.31137828e+00, -1.01297765e+00,\n",
       "         -1.04211089e+00],\n",
       "        [-8.94308978e-01,  2.09934449e-03, -9.43643106e-01,\n",
       "         -1.04211089e+00],\n",
       "        [-7.37687441e-01,  2.12033793e-01, -1.15164673e+00,\n",
       "         -1.04211089e+00],\n",
       "        [ 4.54202458e-02,  8.41837140e-01, -1.08231219e+00,\n",
       "         -1.04211089e+00],\n",
       "        [-8.94308978e-01,  1.05177159e+00, -1.01297765e+00,\n",
       "         -1.21994552e+00],\n",
       "        [-1.67741667e+00, -2.07835104e-01, -1.08231219e+00,\n",
       "         -1.04211089e+00],\n",
       "        [-5.81065904e-01,  6.31902691e-01, -9.43643106e-01,\n",
       "         -1.04211089e+00],\n",
       "        [-7.37687441e-01,  8.41837140e-01, -1.08231219e+00,\n",
       "         -8.64276271e-01],\n",
       "        [-1.52079513e+00, -1.67737625e+00, -1.08231219e+00,\n",
       "         -8.64276271e-01],\n",
       "        [-1.67741667e+00,  2.12033793e-01, -1.08231219e+00,\n",
       "         -1.04211089e+00],\n",
       "        [-7.37687441e-01,  8.41837140e-01, -8.74308565e-01,\n",
       "         -3.30772400e-01],\n",
       "        [-5.81065904e-01,  1.47164049e+00, -6.66304941e-01,\n",
       "         -6.86441647e-01],\n",
       "        [-1.05093052e+00, -2.07835104e-01, -1.01297765e+00,\n",
       "         -8.64276271e-01],\n",
       "        [-5.81065904e-01,  1.47164049e+00, -8.74308565e-01,\n",
       "         -1.04211089e+00],\n",
       "        [-1.36417359e+00,  2.12033793e-01, -1.01297765e+00,\n",
       "         -1.04211089e+00],\n",
       "        [-2.67822829e-01,  1.26170604e+00, -9.43643106e-01,\n",
       "         -1.04211089e+00],\n",
       "        [-7.37687441e-01,  4.21968242e-01, -1.01297765e+00,\n",
       "         -1.04211089e+00],\n",
       "        [ 2.39474331e+00,  2.12033793e-01,  1.27506221e+00,\n",
       "          1.09190459e+00],\n",
       "        [ 1.45501408e+00,  2.12033793e-01,  1.13639313e+00,\n",
       "          1.26973921e+00],\n",
       "        [ 2.23812177e+00,  2.09934449e-03,  1.41373130e+00,\n",
       "          1.26973921e+00],\n",
       "        [ 4.54202458e-02, -1.67737625e+00,  7.89720424e-01,\n",
       "          9.14069966e-01],\n",
       "        [ 1.61163562e+00, -6.27704002e-01,  1.20572767e+00,\n",
       "          1.26973921e+00],\n",
       "        [ 3.58663321e-01, -6.27704002e-01,  1.13639313e+00,\n",
       "          9.14069966e-01],\n",
       "        [ 1.29839254e+00,  4.21968242e-01,  1.27506221e+00,\n",
       "          1.44757384e+00],\n",
       "        [-8.94308978e-01, -1.46744180e+00,  3.04378636e-01,\n",
       "          3.80566095e-01],\n",
       "        [ 1.76825716e+00, -4.17769553e-01,  1.20572767e+00,\n",
       "          9.14069966e-01],\n",
       "        [-4.24444366e-01, -8.37638451e-01,  7.20385883e-01,\n",
       "          1.09190459e+00],\n",
       "        [-7.37687441e-01, -2.30717959e+00,  4.43047718e-01,\n",
       "          3.80566095e-01],\n",
       "        [ 6.71906395e-01, -2.07835104e-01,  9.28389507e-01,\n",
       "          1.26973921e+00],\n",
       "        [ 8.28527933e-01, -1.88731069e+00,  7.89720424e-01,\n",
       "          3.80566095e-01],\n",
       "        [ 9.85149470e-01, -4.17769553e-01,  1.27506221e+00,\n",
       "          1.09190459e+00],\n",
       "        [ 2.02041783e-01, -4.17769553e-01,  5.12382260e-01,\n",
       "          9.14069966e-01],\n",
       "        [ 1.92487869e+00,  2.09934449e-03,  1.06705859e+00,\n",
       "          1.09190459e+00],\n",
       "        [ 2.02041783e-01, -2.07835104e-01,  1.13639313e+00,\n",
       "          1.26973921e+00],\n",
       "        [ 5.15284858e-01, -8.37638451e-01,  8.59054966e-01,\n",
       "          3.80566095e-01],\n",
       "        [ 1.14177101e+00, -1.88731069e+00,  1.13639313e+00,\n",
       "          1.26973921e+00],\n",
       "        [ 2.02041783e-01, -1.25750735e+00,  7.20385883e-01,\n",
       "          5.58400718e-01],\n",
       "        [ 6.71906395e-01,  2.12033793e-01,  1.34439675e+00,\n",
       "          1.80324308e+00],\n",
       "        [ 9.85149470e-01, -6.27704002e-01,  7.89720424e-01,\n",
       "          9.14069966e-01],\n",
       "        [ 1.29839254e+00, -1.25750735e+00,  1.41373130e+00,\n",
       "          1.26973921e+00],\n",
       "        [ 9.85149470e-01, -6.27704002e-01,  1.27506221e+00,\n",
       "          7.36235342e-01],\n",
       "        [ 1.45501408e+00, -4.17769553e-01,  9.97724048e-01,\n",
       "          9.14069966e-01],\n",
       "        [ 1.76825716e+00, -2.07835104e-01,  1.06705859e+00,\n",
       "          1.09190459e+00],\n",
       "        [ 2.08150023e+00, -6.27704002e-01,  1.34439675e+00,\n",
       "          1.09190459e+00],\n",
       "        [ 1.92487869e+00, -2.07835104e-01,  1.48306584e+00,\n",
       "          1.62540846e+00],\n",
       "        [ 8.28527933e-01, -4.17769553e-01,  1.13639313e+00,\n",
       "          1.26973921e+00],\n",
       "        [ 3.58663321e-01, -1.04757290e+00,  4.43047718e-01,\n",
       "          3.80566095e-01],\n",
       "        [ 4.54202458e-02, -1.46744180e+00,  6.51051342e-01,\n",
       "          5.58400718e-01],\n",
       "        [ 4.54202458e-02, -1.46744180e+00,  5.81716801e-01,\n",
       "          3.80566095e-01],\n",
       "        [ 5.15284858e-01, -8.37638451e-01,  7.20385883e-01,\n",
       "          7.36235342e-01],\n",
       "        [ 8.28527933e-01, -8.37638451e-01,  1.55240038e+00,\n",
       "          1.44757384e+00],\n",
       "        [-1.11201292e-01, -2.07835104e-01,  1.13639313e+00,\n",
       "          1.26973921e+00],\n",
       "        [ 8.28527933e-01,  6.31902691e-01,  1.13639313e+00,\n",
       "          1.44757384e+00],\n",
       "        [ 1.92487869e+00,  2.09934449e-03,  1.27506221e+00,\n",
       "          1.26973921e+00],\n",
       "        [ 1.29839254e+00, -1.67737625e+00,  1.06705859e+00,\n",
       "          9.14069966e-01],\n",
       "        [ 2.02041783e-01, -2.07835104e-01,  8.59054966e-01,\n",
       "          9.14069966e-01],\n",
       "        [ 4.54202458e-02, -1.25750735e+00,  7.89720424e-01,\n",
       "          9.14069966e-01],\n",
       "        [ 4.54202458e-02, -1.04757290e+00,  1.06705859e+00,\n",
       "          7.36235342e-01],\n",
       "        [ 9.85149470e-01, -2.07835104e-01,  1.20572767e+00,\n",
       "          1.09190459e+00],\n",
       "        [ 5.15284858e-01, -1.04757290e+00,  7.89720424e-01,\n",
       "          7.36235342e-01],\n",
       "        [-7.37687441e-01, -1.67737625e+00,  3.04378636e-01,\n",
       "          3.80566095e-01],\n",
       "        [ 2.02041783e-01, -8.37638451e-01,  9.28389507e-01,\n",
       "          9.14069966e-01],\n",
       "        [ 3.58663321e-01, -2.07835104e-01,  9.28389507e-01,\n",
       "          7.36235342e-01],\n",
       "        [ 3.58663321e-01, -4.17769553e-01,  9.28389507e-01,\n",
       "          9.14069966e-01],\n",
       "        [ 1.14177101e+00, -4.17769553e-01,  9.97724048e-01,\n",
       "          9.14069966e-01],\n",
       "        [-5.81065904e-01, -1.25750735e+00,  9.63750123e-02,\n",
       "          5.58400718e-01],\n",
       "        [ 3.58663321e-01, -6.27704002e-01,  8.59054966e-01,\n",
       "          9.14069966e-01]]),\n",
       " array([[5.471, 3.099, 2.861, 0.786]]),\n",
       " array([[0.63848179, 0.47633916, 1.44228257, 0.56232019]]))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "standarize_data(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teraz przejdźmy do utworzenia funkcji `split_data`, która ma za zadanie podzielić dostarczony jej zbiór danych X (i etykiety y) na zestaw treningowy i testowy, zgodnie z podanym jako argument procentem (wartość `percentage_train` odpowiadać ma procentowi, jaką częścią oryginalnego zbioru danych ma być zbiór treningowy).\n",
    "\n",
    "<font size=\"2\">Wskazówki przydatne przy implementacji funkcji `split_data`:\n",
    "* Zapoznaj się z dokumentacją funkcji `np.random.choice` z biblioteki NumPy ([TUTAJ](https://numpy.org/doc/stable/reference/random/generated/numpy.random.choice.html)). Zwróć uwagę na parametr `replace`.\n",
    "* Zapoznaj się z trickiem na usunięcie elementów jednej listy z drugiej z wykorzystaniem różnicy zbiorów (więcej o tym możesz przeczytać [TUTAJ](https://stackoverflow.com/questions/3428536/how-do-i-subtract-one-list-from-another/)). Do utworzenia listy indeksów wszystkich rekordów danych możesz użyć funkcji `np.arange` z biblioteki NumPy ([TUTAJ](https://numpy.org/doc/stable/reference/generated/numpy.arange.html)).\n",
    "* Zapoznaj się ze sposobami na wyodrębnienie części tablicy w NumPy ([TUTAJ](https://numpy.org/doc/stable/user/basics.indexing.html)). Możesz je filtrować według elementów innej listy!\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(X, y, percentage_train = 70.0, seed=100):\n",
    "    \"\"\"Funkcja dzieląca losowo dane na zestaw treningowy oraz testowy w zadanej proporcji. \\n\n",
    "    \n",
    "    Argumenty: \\n\n",
    "      - X - dane wejściowe (numpy array, shape = (num_samples, num_features) ), \\n\n",
    "      - y - etykiety (numpy array, shape = (num_samples,) ), \\n\n",
    "      - percentage_train (argument opcjonalny) - jakim procentem wejściowych danych ma być \n",
    "        zestaw treningowy (skalar, float, domyślna wartość: 70). \\n\n",
    "    Zwraca: \\n\n",
    "      - Xtrain - dane treningowe (numpy array, shape = (num_samples * percentage_train, num_features) ), \\n\n",
    "      - ytrain - etykiety do danych treningowych (numpy array, shape = (num_samples * percentage_train,) ), \\n\n",
    "      - Xtest - dane testowe (numpy array, shape = (num_samples * (100-percentage_train), num_features) ), \\n\n",
    "      - ytest - etykiety do danych testowych (numpy array, shape = (num_samples * (100-percentage_train),) ).\"\"\"\n",
    "    \n",
    "    np.random.seed(seed) # Dla zapanowania nad \"losowością\"\n",
    "    num_all_datapoints =  X.shape[0] # Ilość wszystkich danych\n",
    "    num_train_datapoints = int(np.round(X.shape[0]*percentage_train/100)) # Docelowa wielkość zestawu treningowego\n",
    "    \n",
    "    # ------- UZUPEŁNIJ KOD --------\n",
    "    # Znając rozmiar danych wejściowych, wygeneruj indices_train - listę indeksów tych rekordów, \n",
    "    # które mają należeć do zestawu treningowego\n",
    "    indices_train = np.random.choice(X.shape[0], size=num_train_datapoints, replace=False)\n",
    "    ind = np.zeros(X.shape[0], dtype=bool) # https://stackoverflow.com/questions/50491630/randomly-split-a-numpy-array\n",
    "    ind[indices_train] = True\n",
    "    indices_test = ~ind\n",
    "    # Wygeneruj zmienne z właściwie podzielonym zbiorem danych: Xtrain, ytrain, Xtest, ytest\n",
    "    Xtrain = X[indices_train]\n",
    "    ytrain = y[indices_train]\n",
    "    Xtest = X[indices_test]\n",
    "    ytest = y[indices_test]\n",
    "    # ------------------------------\n",
    "    \n",
    "    return Xtrain, ytrain, Xtest, ytest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pora sprawdzić, jak działają napisane przez Ciebie funkcje! Uruchom poniższy kod, aby podzielić nasz zbiór danych na zestaw treningowy (powinien domyślnie zawierać 70 elementów) i testowy (pozostałe 30 elementów). Elementy w poszczególnych cechach powinny po normalizacji oscylować wokół 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wymiary danych treningowych: (70, 4)\n",
      "Wymiary danych testowych: (30, 4)\n",
      "Przykładowe znormalizowane dane treningowe: \n",
      "[[-0.97023354  1.12689892 -1.11317579 -1.32513741]\n",
      " [ 0.73542816 -1.9493154   0.67340264  0.2589927 ]\n",
      " [-0.81517338  0.68743973 -0.97574668 -0.79709404]\n",
      " [-1.59047416 -1.7295858  -1.18189035 -0.9731085 ]\n",
      " [-0.81517338  0.24798054 -1.2506049  -1.14912295]]\n"
     ]
    }
   ],
   "source": [
    "# Podział danych na część treningową i testową\n",
    "Xtrain, ytrain, Xtest, ytest = split_data(X ,y)\n",
    "print(\"Wymiary danych treningowych: \"+ str(Xtrain.shape))\n",
    "print(\"Wymiary danych testowych: \"+ str(Xtest.shape))\n",
    "\n",
    "# Normalizacja obu zestawów danych\n",
    "Xtrain_norm, _, _ = standarize_data(Xtrain) # pomijamy zwracanie mu i sigma dla danych treningowych\n",
    "Xtest_norm, mu_test, sigma_test = standarize_data(Xtest)\n",
    "print(\"Przykładowe znormalizowane dane treningowe: \")\n",
    "print(Xtrain_norm[0:5,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trening modelu regresji logistycznej\n",
    "\n",
    "Teraz, kiedy mamy już gotowe dane, na których możemy pracować, przejdźmy do najważniejszej rzeczy, czyli napisania funkcji, które utworzą nasz model oparty na regresji logistycznej i pozwolą mu się uczyć!\n",
    "\n",
    "Przypomnijmy, że **hipotezą** $h_\\theta(x)$ (tj. funkcją, która wiąże parametry modelu i dane wejściowe, dając w wyniku predykcje) regresji logistycznej jest: \n",
    "\\begin{equation*}\n",
    "    h_\\theta(x) = g(\\theta^Tx)\n",
    "\\end{equation*}\n",
    "gdzie funkcja $g(z)$ jest to tzw. funkcja **sigmoid** o następującej postaci:\n",
    "\\begin{equation*}\n",
    "    g(z) = \\frac{1}{1+e^{-z}}\n",
    "\\end{equation*}\n",
    "\n",
    "<div align=\"center\">\n",
    "\n",
    "<img src='https://miro.medium.com/v2/resize:fit:1280/1*OUOB_YF41M-O4GgZH_F2rw.png' />\n",
    "\n",
    "<font size=\"1\">Grafika: towardsdatascience.com</font>\n",
    "</div>\n",
    "\n",
    "Funkcja sigmoid ma kilka ciekawych właściwości, dzięki którym jest często spotykana przy rozwiązywaniu problemów klasyfikacji: jej wartości zawierają się w przedziale od 0 do 1 (dlatego można je traktować jak prawdopodobieństwo należenia danego rekordu danych do pewnej klasy i np. traktować te dane, dla których sigmoid zwrócił wartość większą niż 0,5, jako należące do tejże klasy), a także jest odwracalna i różniczkowalna, dzięki czemu da się obliczać jej gradient niezbędny w procesie uczenia modelu. Zaimplementuj zatem funkcję `sigmoid`, która zwraca wartość sigmoidu dla dowolnej *numpy array*!\n",
    "\n",
    "<font size=\"2\">Wskazówka przydatna przy implementacji funkcji `sigmoid`: Warto skorzystać z funkcji `np.exp` z biblioteki NumPy ([TUTAJ](https://numpy.org/doc/stable/reference/generated/numpy.exp.html))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"Funkcja obliczająca wartość sigmoidu dla zadanego argumentu z. \\n\n",
    "    \"\"\"\n",
    "    # ------- UZUPEŁNIJ KOD --------\n",
    "    sigmoid = 1/(1 + np.exp(-z)) \n",
    "    # ------------------------------\n",
    "    return sigmoid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sprawdź poprawność implementacji funkcji `sigmoid`. Obliczymy, co zwraca ta funkcja, gdy argumentem są same zera: skalar, wektor 1-D oraz macierz 2-D. W każdym przypadku, sigmoid powinien zwrócić stukturę o takich samych wymiarach, jak dane wejściowe, a każdy z jej elementów powinien wynosić 0,5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sigmoid dla skalara: 0.5\n",
      "Sigmoid dla wektora: [0.5 0.5 0.5]\n",
      "Sigmoid dla macierzy: [[0.5 0.5 0.5]\n",
      " [0.5 0.5 0.5]\n",
      " [0.5 0.5 0.5]]\n"
     ]
    }
   ],
   "source": [
    "# Testowanie poprawności implementacji sigmoidu\n",
    "print(\"Sigmoid dla skalara: \" + str(sigmoid(0)))\n",
    "print(\"Sigmoid dla wektora: \" + str(sigmoid(np.zeros((3)))))\n",
    "print(\"Sigmoid dla macierzy: \" + str(sigmoid(np.zeros((3,3)))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jako funkcję kosztu $J(\\theta)$ użyjemy **binarnej entropii krzyżowej** (ang. *Binary Cross Entropy*, BCE), często spotykaną przy okazji problemów klasyfikacji binarnej. Jej wartość jest tym większa, im więcej pomyłek popełni klasyfikator: przy zgodności etykiety $y^{(i)}$ i predykcji $h_\\theta(x^{(i)})$, oba człony wyrażenia zerują się. Funkcja ta opisana jest wzorem:\n",
    "\\begin{equation*}\n",
    "\tJ(\\theta) = \\frac{1}{m} \\sum_{i=1}^{m} [-y^{(i)}\\log(h_\\theta(x^{(i)})) - (1-y^{(i)})\\log(1-h_\\theta(x^{(i)}))]\t\n",
    "\\end{equation*}\n",
    "\n",
    "Jak już wiesz, trening modelu opiera się na znalezieniu optymalnych parametrów $\\theta$, tj. takich, przy których funkcja kosztu jest minimalna. My taką optymalizację przeprowadzimy z wykorzystaniem metody gradientu prostego, która do poprawnego działania musi znać gradient $\\frac{\\partial J(\\theta)}{\\partial \\theta}$ naszej funcji kosztu. Pamiętaj, że **gradient ma taki sam wymiar, jak wektor parametrów $\\theta$**, a zatem składa się z $n$ elementów. W przypadku gradientu fukcji BCE, każdy z jego $n$ elementów można obliczyć z następującego wzoru (pomijam tutaj jego wyprowadzenie):\n",
    "\\begin{equation*}\n",
    "\t\\frac{\\partial J(\\theta)}{\\partial \\theta_n} = \\frac{1}{m} \\sum_{i=1}^{m} (h_\\theta(x^{(i)})-y^{(i)}) \\cdot x^{(i)}_n\n",
    "\\end{equation*}\n",
    "\n",
    "Napisz zatem funkcję `compute_cost_and_gradient`, w której na podstawie danych treningowych i zadanych parametrów $\\theta$, obliczysz koszt BCE i jego gradient.\n",
    "\n",
    "<font size=\"2\">Wskazówki przydatne przy implementacji funkcji `compute_cost_and_gradient`:\n",
    "* Przy obliczaniu kosztu, będzie Ci na pewno potrzebna funkcja `np.log2` z biblioteki NumPy ([TUTAJ](https://numpy.org/doc/stable/reference/generated/numpy.log2.html)) oraz napisana przez Ciebie wcześniej funkcja `sigmoid`.\n",
    "* Zamiast używania pętli `for` do iteracji po wszystkich $m$ elementach, możesz wykonać operacje na macierzach (z wykorzystaniem funkcji `np.dot` ([TUTAJ](https://numpy.org/doc/stable/reference/generated/numpy.dot.html))). Pamiętaj jednak o właściwościach mnożenia macierzy - nie każde macierze da się przemnożyć, muszą one mieć zgodne \"wewnętrze\" wymiary (ilość kolumn pierwszej macierzy musi być taka sama, jak ilość wierszy drugiej macierzy; wówczas w wyniku mnożenia otrzymujemy macierz o zgodnych \"zewnętrznych\" wymiarach, tj. o liczbie wierszy jak pierwsza macierz i liczbie kolumn jak druga macierz: [M,N]x[N,1]=[M,1]): w razie potrzeby zmień kolejność mnożonych macierzy albo dokonaj ich transpozycji z użyciem funkcji `np.transpose` ([TUTAJ](https://numpy.org/doc/stable/reference/generated/numpy.transpose.html)).\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost_and_gradient(X, y, theta):\n",
    "    \"\"\"Funkcja obliczająca koszt BCE dla regresji logistycznej. \\n\n",
    "    Argumenty: \\n\n",
    "    X - dane wejściowe (numpy array, shape = (num_samples, num_features) ), \\n\n",
    "    y - etykiety (numpy array, shape = (num_samples,) ), \\n\n",
    "    theta - zestaw parametrów (numpy array, shape = (num_features,) ). \\n\n",
    "    Zwraca: \\n\n",
    "    J - obliczony koszt (skalar, float), \\n\n",
    "    grad - obliczony gradient funkcji kosztu (numpy array, shape=(num_features) ). \"\"\"\n",
    "    \n",
    "    # ------- UZUPEŁNIJ KOD --------\n",
    "    # Oblicz wartość funkcji kosztu\n",
    "     \n",
    "\n",
    "   \n",
    "\n",
    "    J =  np.dot((1 / X.shape[0]),  (np.dot(-y, np.log2(sigmoid(X))) - np.dot((1 - y), np.log2(1 - sigmoid(X))))) \n",
    "    # Oblicz jej gradient\n",
    "    grad = 0\n",
    "    # ------------------------------\n",
    "    \n",
    "    return J, grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sprawdźmy poprawność implementacji powyższej funkcji: uruchom następującą komórkę, aby wyliczyć przykładowe wartości kosztu i jego gradientu dla zerowych parametrów (kiedy wszystkie elementy $\\theta$ są równe 0), liczone dla całego naszego zbioru danych. Koszt powinien wynosić 1, a gradient [-0.2325,  0.1645, -0.6995, -0.27]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Przykładowy koszt i jego gradient, liczony dla zerowych parmetrów: (array([361.61962854, 249.7871097 , 120.67149185,  59.68502634]), 0)\n"
     ]
    }
   ],
   "source": [
    "# Testowanie poprawności implementacji funkcji kosztu\n",
    "print(\"Przykładowy koszt i jego gradient, liczony dla zerowych parmetrów: \"+str(compute_cost_and_gradient(\n",
    "    X,y,np.zeros((X.shape[1])))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mając wyznaczoną funkcję kosztu, możemy przejść do jej optymalizacji, czyli doboru takich wartości parametrów $\\theta$, które dają najniższą wartość kosztu (tj. najlepiej odwzorowują dane treningowe). Jak już wspomniano, zrobimy to z wykorzystaniem metody gradientu prostego, według której aktualizacja parametrów odbywa się według poniższego wzoru:\n",
    "\\begin{equation*}\n",
    "    \\theta_n := \\theta_n - \\alpha \\cdot \\frac{\\partial J(\\theta)}{\\partial \\theta_n}\n",
    "\\end{equation*}\n",
    "gdzie hiperparametr $\\alpha$ (którego wartość musi być zdefiniowana zanim przejdzie się do treningu modelu), oznacza, jak bardzo gradient funkcji kosztu ma wpływ na nową, zaktualizowaną postać parametrów $\\theta$\n",
    "\n",
    "Napisz zatem funkcję `train_logistic_regression`, w której na podstawie danych treningowych, iteracyjnie liczona jest wartość funkcji kosztu i jego gradient, parametry są aktualizowane zgodnie z metodą gradientu prostego, a ponadto przy każdej itaracji wizualizowana jest nowa wartość kosztu, aby móc ocenić, czy w ramach treningu koszt rzeczywiście spada (ważne - jeśli zaobserwowalibyśmy wzrost kosztu wraz z kolejnymi  iteracjami, oznacza to, że model CORAZ GORZEJ radzi sobie z analizą danych treningowych, a zatem wcale się nie uczy!). \"Szkielet\" tej funkcji został już napisany, wykonanych zostanie 400 iteracji, a stała uczenia może zostać w postaci domyślnej (ustawionej na 0,01).\n",
    "\n",
    "<font size=\"2\">Poprawność implementacji tej funkcji sprawdzimy nieco później.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_logistic_regression(X,y,alpha=0.01):\n",
    "    \"\"\"Funkcja realizująca optymalizację funkcji kosztu dla regresji logistycznej\n",
    "    w celu wytrenowania modelu (otrzymania zestawu najlepszych parametrów, theta)\n",
    "    z wykorzystaniem metody gradientu prostego. \\n\n",
    "    Argumenty: \\n\n",
    "    X - dane treningowe (numpy array, shape = (num_samples, num_features) ), \\n\n",
    "    y - etykiety (numpy array, shape = (num_samples,) ), \\n\n",
    "    alpha (opcjonalnie) - stała uczenia (skalar, float, domyślnie 0.001). \\n\n",
    "    Zwraca: theta - zestaw optymalnych parametrów (numpy array, shape = (num_features,) ). \"\"\"\n",
    "    \n",
    "    # Inicjalizacja\n",
    "    num_iterations = 400 # tyle razy wykonać ma się gradient descent\n",
    "    theta = np.zeros((X.shape[1])) # wstępna inicjalizacja parametrów samymi zerami\n",
    "    Js = np.zeros(num_iterations) # wektor przechowujący dotychczasowe wartości kosztu (do wizualizacji)\n",
    "    \n",
    "    # Uruchomienie metody gradientów prostych\n",
    "    print(\"\\nTrwa trening modelu... \")\n",
    "    for i in range(num_iterations):\n",
    "        # ------- UZUPEŁNIJ KOD --------\n",
    "        # Korzystając z wcześniej napisanej fukcji, oblicz funkcję kosztu J i jej gradient grad\n",
    "        J, grad = 0\n",
    "        # Zapisz obliczony koszt jako odpowiedni element w Js\n",
    "\n",
    "        # Dokonaj aktualizacji parametrów theta o wcześniej obliczony gradient przemnożony przez stałą uczenia\n",
    "        \n",
    "        # ------------------------------\n",
    "    print(\"Zakończono. \")\n",
    "    \n",
    "    # Wizualizacja zmian kosztu\n",
    "    plt.figure()\n",
    "    plt.plot(Js)\n",
    "    plt.title(\"Efekty treningu modelu - zmiany w koszcie\")\n",
    "    plt.xlabel(\"Numer iteracji\")\n",
    "    plt.ylabel(\"Wartość funkcji kosztu\")\n",
    "    plt.show(block=False)\n",
    "    \n",
    "    return theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predykcja i poskładanie wszystkiego w całość!\n",
    "\n",
    "Wytrenowany model musi umieć dokonywać predykcji - w tym przypadku, podejmować decyzję, czy analizowany rekord danych zaklasyfikować do klasy pierwszej (etykieta 0) czy drugiej (etykieta 1). Napisz zatem ostatnią w tej części ćwiczenia funkcję, `predict_logistic_regression`, która oblicza funkcję hipotezy dla regresji logistycznej i zwraca odpowiednie etykiety.\n",
    "\n",
    "<font size=\"2\">Wskazówka przydatna przy implementacji funkcji `predict_logistic_regression`: Pamiętaj, że model ma zwrócić etykietę 0 w sytuacji, kiedy prawdopodobieństwo obliczone z wykorzystaniem hipotezy jest mniejsze niż 0,5. Poszukaj funkcji z biblioteki NumPy, która realizuje przybliżanie do najbliższej liczby naturalnej!\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_logistic_regression(X,theta):\n",
    "    \"\"\"Funkcja obliczająca ostateczną predykcję regresji logistycznej. \\n\n",
    "    Argumenty: \\n\n",
    "    X - dane wejściowe (numpy array, shape = (num_samples, num_features) ), \\n\n",
    "    theta - zestaw optymalnych parametrów (numpy array, shape = (num_features,) ). \\n\n",
    "    Zwraca: pred - dokonane predykcje (numpy array, shape = (num_samples,) ). \"\"\"\n",
    "    # ------- UZUPEŁNIJ KOD --------\n",
    "    pred = 0\n",
    "    # ------------------------------\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sprawdź poprawność implementacji funkcji `predict_logistic_regression`. W poniższej komórce wykonujemu tę funkcję dla pierwszych pięciu rekordów oryginalnego zbioru danych i zerowych parametrów. Powinieneś otrzymać w wyniku same zera."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Przykładowe predykcje przy zerowych parametrach: 0\n"
     ]
    }
   ],
   "source": [
    "# Testowanie poprawności implementacji predykcji\n",
    "print(\"Przykładowe predykcje przy zerowych parametrach: \"+str(predict_logistic_regression(X[0:5,:],np.zeros((X.shape[1])))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Czas uruchomić całość - od właściwego uczenia naszego modelu, aż po dokonanie przezeń predykcji na danych testowych! Uruchom poniższy kod, w którym wykorzystujemy niemal wszystko, co do tej pory udało nam się napisać. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Xtrain_norm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/kuba/Labs/Lab_01_Regresja_logistyczna.ipynb Cell 27\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/kuba/Labs/Lab_01_Regresja_logistyczna.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Trening modelu (na danych treningowych)\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/kuba/Labs/Lab_01_Regresja_logistyczna.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m theta \u001b[39m=\u001b[39m train_logistic_regression(Xtrain_norm,ytrain)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/kuba/Labs/Lab_01_Regresja_logistyczna.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# Dokonanie predykcji (na danych testowych) i obliczenie dokładności modelu\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/kuba/Labs/Lab_01_Regresja_logistyczna.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m pred \u001b[39m=\u001b[39m predict_logistic_regression(Xtest_norm,theta)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Xtrain_norm' is not defined"
     ]
    }
   ],
   "source": [
    "# Trening modelu (na danych treningowych)\n",
    "theta = train_logistic_regression(Xtrain_norm,ytrain)\n",
    "# Dokonanie predykcji (na danych testowych) i obliczenie dokładności modelu\n",
    "pred = predict_logistic_regression(Xtest_norm,theta)\n",
    "accuracy = np.mean(pred==ytest)\n",
    "print(\"Dokładność modelu na danych testowych: \"+str(accuracy*100)+'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jeśli na wykresie obserwujesz stopniowy spadek kosztu - świetnie, Twój model się uczy! Można też zaobserwować bardzo wysoką wartość dokładności (**dokładność** to procent decyzji, które model podjął właściwie) naszego modelu na danych z zestawu testowego, świadczącą o jego poprawnym działaniu. \n",
    "\n",
    "\n",
    "## 4.2. Uruchomienie klasyfikatorów z biblioteki Scikit-learn\n",
    "\n",
    "### Import nowych niezbędnych klas\n",
    "\n",
    "Udało Ci się stworzyć model klasyfikatora opartego o regresję logistyczną od zera. Na szczęście nie zawsze trzeba włożyć tyle pracy, aby móc używać algorytmów uczenia maszynowego. Istnieją biblioteki posiadające gotowe implementacje wielu z nich. Jedną z takich bibliotek jest wykorzystywana już przez Ciebie biblioteka Scikit-learn. Tym razem do sprawdzenia, jak z analizowanym tutaj zbiorze danych iris radzą sobie inne algorytmy, wykorzystamy jej gotowe klasy i metody:\n",
    "- model regresji logistycznej - `linear_model.LogisticRegression` (dokumentacja [TUTAJ](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)), \n",
    "- model drzewa decyzyjnego - `tree.DecisionTreeClassifier` ([TUTAJ](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)),\n",
    "- model *k*-NN - `neighbors.KNeighborsClassifier` ([TUTAJ](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html)),\n",
    "- metrykę dokładności - `metrics.accuracy_score` ([TUTAJ](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html)),\n",
    "- klasę standaryzującą dane - `preprocessing.StandardScaler` ([TUTAJ](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)),\n",
    "- funkcję dzielącą dane na zestaw treningowy i testowy - `model_selection.train_test_split` ([TUTAJ](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)).\n",
    "\n",
    "Uruchom poniższą komórkę, aby zaimportować niezbędne klasy i metody. Zapoznaj się też z ich dokumentacją."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Przygotowanie danych z użyciem gotowych metod\n",
    "\n",
    "Mając wciąż w pamięci przechowywane zmienne X i y z oryginalnym, jeszcze nieznormalizowanym ani niepodzielonym zbiorem danych iris (oraz etykietami), możemy je ponownie wykorzystać w tej części ćwiczenia. Napisz więc fragment kodu, który:\n",
    "* dzieli dane X i y na właściwe zestawy Xtrain i Xtest (użyj metody `train_test_split`) - niech zestaw treningowy zawiera 70% danych,\n",
    "* standaryzuje Xtrain i Xtest (zwróć uwagę na metodę `fit_transform` klasy `StandardScaler`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------ UZUPEŁNIJ KOD -------------\n",
    "# Podziel dane z X i y na zestaw treningowy (70%) i testowy (30%) z wykorzystaniem metody train_test_split\n",
    "Xtrain, Xtest, ytrain, ytest = 0\n",
    "# Dokonaj normalizacji danych treningowych z wykorzystaniem klasy StandardScaler - zwróć Xtrain_norm\n",
    "scaler = StandardScaler()\n",
    "Xtrain_norm = 0\n",
    "# Dokonaj normalizacji danych testowych - zwróć Xtest_norm\n",
    "Xtest_norm = 0\n",
    "# ----------------------------------------\n",
    "\n",
    "print(\"Wymiary danych treningowych: \"+str(Xtrain.shape))\n",
    "print(\"Wymiary danych testowych: \"+str(Xtest.shape))\n",
    "print(\"Przykładowe znormalizowane dane treningowe: \")\n",
    "print(Xtrain_norm[0:5,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regresja logistyczna z biblioteki Scikit-learn\n",
    "\n",
    "Mamy przygotowane dane, pora zatem wytrenować model regresji logistycznej z wykorzystaniem klasy `LogisticRegression`!\n",
    "* Do trenowania (na bazie zestandaryzowanego zestawu treningowego) użyj metody `fit`.\n",
    "* Do dokonania predykcji użyj metody `predict` - zobacz, jakich predykcji dokona Twój model na widok danych z zestandaryzowanego zestawu testowego (zapisz je do zmiennej o nazwie `pred`).\n",
    "* Oblicz dokładność predykcji Twojego modelu (dla zestandaryzowanych danych testowych) z wykorzystaniem metody `accuracy_score`. Wynik zapisz do zmiennej o nazwie `acccuracy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression() # zostawmy domyślne ustawienia\n",
    "# ------------ UZUPEŁNIJ KOD -------------\n",
    "# Wytrenuj model regresji logistycznej na zestandaryzowanych danych treningowych\n",
    "\n",
    "# Zwróć predykcję modelu (do zmiennej o nazwie pred) dla  zestandaryzowanych danych testowych\n",
    "pred = 0\n",
    "# Oblicz dokładność predykcji\n",
    "accuracy = 0\n",
    "# ----------------------------------------\n",
    "print(\"Dokładność modelu na danych testowych: \"+str(accuracy*100)+'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inne klasyfikatory dostępne w Scikit-learn\n",
    "\n",
    "Na sam koniec, sprawdź, jak zachowują się inne modele klasyfikatorów, dostępne w ramach biblioteki Scikit-learn: drzewo decyzyjne oraz *k*-NN! Uruchom po prostu poniższy kod:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drzewo decyzyjne\n",
    "model2 = DecisionTreeClassifier(max_depth=5)\n",
    "model2.fit(Xtrain_norm,ytrain)\n",
    "pred = model2.predict(Xtest_norm)\n",
    "accuracy = accuracy_score(ytest,pred)\n",
    "print(\"Dokładność drzewa decyzyjnego na danych testowych: \"+str(accuracy*100)+'%')\n",
    "\n",
    "# k-NN\n",
    "model3 = KNeighborsClassifier(n_neighbors=3)\n",
    "model3.fit(Xtrain_norm,ytrain)\n",
    "pred = model3.predict(Xtest_norm)\n",
    "accuracy = accuracy_score(ytest,pred)\n",
    "print(\"Dokładność k-NN na danych testowych: \"+str(accuracy*100)+'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gratulacje! W ten sposób zakończyliśmy to ćwiczenie, w którym skupiliśmy się na działaniu klasycznych algorytmów klasyfikacji.\n",
    "\n",
    "\n",
    "## 5. Pytania kontrolne\n",
    "\n",
    "1. Czym różni się uczenie nadzorowane od nienadzorowanego?\n",
    "2. Jakie są różnice pomiędzy problemem regresji a klasyfikacji?\n",
    "3. Opisz krótko, na czym polega trening modelu uczenia maszynowego z wykorzystaniem metody gradientów prostych."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0d1a1735bd163b32102b8ea4514749acd69bcad8d489552073ec2e1a14a559f4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
