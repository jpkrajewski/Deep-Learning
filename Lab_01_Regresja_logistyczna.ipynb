{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "\n",
    "<font size=\"5\">\n",
    "\n",
    "Laboratorium z przedmiotu: \\\n",
    "**Głębokie uczenie i analiza obrazów**\n",
    "\n",
    "Ćwiczenie 1: \\\n",
    "**Regresja logistyczna i inne klasyczne algorytmy klasyfikacji**\n",
    "\n",
    "</font>\n",
    "\n",
    "\\\n",
    "Marta Szarmach \\\n",
    "Zakład Telekomunikacji Morskiej \\\n",
    "Wydział Elektryczny \\\n",
    "Uniwersytet Morski w Gdyni\n",
    "\n",
    "07.2023\n",
    "</div>\n",
    "\n",
    "\n",
    "# 1. Wprowadzenie\n",
    "\n",
    "**Uczenie maszynowe** jest to zbiór algorytmów, które odnajdują zależności ukryte w danych, potrafią te zależności modelować (tj. opisywać za pomocą matematycznych struktur) bez bycia jawnie zaprogramowanym przez człowieka i doskonalą swoje działanie (tj. potrafią dobierać lepsze parametry opisujące modele) dzięki dostarczeniu do nich nowych danych (uczenie maszynowe jest jedynie częścią szerszego zagadnienia, jakim jest **sztuczna inteligencja**, tj. dziedzina nauki z pogranicza informatyki, matematyki, kongwinistyki i neurologii, zajmująca się tworzeniem maszyn/oprogramowania ,,udających'' ludzką inteligencję, tj. potrafiącego analizować dostarczone doń dane, wyciągać na ich podstawie wnioski i podejmować decyzje).  Innymi słowy, w przeciwieństwie do klasycznego programowania, nie tworzy się gotowych reguł, lecz algorytm sam, na podstawie dostarczonych danych i spodziewanych odpowiedzi na nie, określa reguły, tzn. tworzy **model**, który stara się odzwierciedlić strukturę danych i sposób wyznaczania tychże odpowiedzi. Model najczęściej opisany jest za pomocą fukcji (hipotezy) zależnej od **parametrów** $\\theta$, które są pewnymi wartościami liczbowymi ulegającymi zmianie w ramach treningu (nie należy mylić parametrów z **hiperparametrami**, które również opisują w pewnym sensie model, lecz nie są wyznaczane podczas treningu, a wręcz przeciwnie, muszą być podane wcześniej).\n",
    "\n",
    "Aby stworzyć i wytrenować model, najczęściej określa się pewną funkcję (nazywaną **funkcją kosztu** $J(\\theta)$), która jest w stanie policzyć, jak bardzo model myli się podczas dokonywania predykcji. **Trening** polega na optymalizacji funkcji kosztu, a dokładniej mówiąc, iteracyjnej aktualizacji parametrów modelu, aż koszt na danych treningowych staje się (najczęściej) minimalny - wówczas uważa się, że model możliwie najlepiej odzwierciedla te dane. Jedną z metod optymalizacji jest **metoda gradientu prostego**, której działanie opiera się na poszukiwaniu lokalnego minimum poprzez wyznaczanie gradientu $\\frac{\\partial J(\\theta)}{\\partial \\theta}$ (kierunku najszybszej zmiany) funkcji kosztu i aktualizacji parametrów zgodnie z tym gradientem. \n",
    "\n",
    "\\\n",
    "Uczenie maszynowe można podzielić na dwie podstawowe grupy: \n",
    "* **uczenie nadzorowane**, w którym każdy rekord danych dostarczony do algorytmu posiada **etykietę** zawierającą pożądaną odpowiedź algorytmu na ten konkretny rekord. Algorytmy uczenia nadzorowanego potrafią rozwiązywać takie problemy jak:\n",
    "    * **regresja** - przypisanie do rekordu danych pewnej dowolnej liczby,\n",
    "    * **klasyfikacja** -  przypisanie do rekordu danych liczby (z przedziału dyskretnego), symbolizującej jego przynależność do pewnej klasy,\n",
    "* **uczenie nienadzorowane** - dane nie posiadają predefiniowanych etykiet, algorytmy muszą same znaleźć strukturę w danych. Rozwiązują takie problemy, jak m.in.:\n",
    "    * **grupowanie** - podział zebranych danych na grupy, tak, aby dane z jednej grupy były bardziej podobne do siebie niż do danych z innych grup,\n",
    "    * **wykrywanie anomalii** - odnalezienie w zbiorze danych tych rekordów, które w pewien sposób odróżniają się od reszty.\n",
    "\n",
    "W niniejszym ćwiczeniu zajmować się będziemy jedynie zagadnieniem klasyfikacji. Niektórymi algorytmami uczenia maszynowego realizującymi klasyfikację są:\n",
    "* **Maszyna wektrów wspierających** (ang. *Support Vector Machine*, **SVM**) - algorytm, który dokonuje klasyfikacji danych poprzez utworzenie (z pomocą dodatkowych funkcji, tzw. kerneli) dodatkowego wymiaru i hiperpłaszczyzny, która oddziela na tym wymiarze dane z różnych klas z maksymalnym możliwym marginesem,\n",
    "* **Drzewo decyzyjne** (ang. *Decision Tree*) - zbiór hierarchicznie następujących po sobie instrukcji warunkowych, których ostatnia warstwa decyduje o wyniku predykcji,\n",
    "* *k* **najbliższych sąsiadów** (ang. *k Nearest Neighbours*, *k*-NN) - to, jaka zostanie podjęta decyzja dotycząca badanego rekordu, zależy od etykiet $k$ innych rekordów najbliższych temu rekordowi,\n",
    "* a także **regresja logistyczna** - która to będzie głównym zagadnieniem niniejszego ćwiczenia. W toku działania tegoż algorytmu, dokonuje się dopasowania pewnej funkcji, wiążącej parametry $\\theta$ i dane X, której zadaniem jest oszacowanie prawdopodobieństwa, z jakim określony rekord danych należy do pewnej klasy.\n",
    "\n",
    "<div align=\"center\">\n",
    "\n",
    "<img src='https://miro.medium.com/v2/resize:fit:828/1*PQ8tdohapfm-YHlrRIRuOA.gif' />\n",
    "\n",
    "<font size=\"1\">Grafika: towardsdatascience.com</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Cel ćwiczenia\n",
    "\n",
    "**Celem niniejszego ćwiczenia** jest zapoznanie się z działaniem klasycznych algorytmów uczenia maszynowego realizujących zagadnienia klasyfikacji poprzez:\n",
    "* implementacji ,,od zera'' algorytmu regresji logistycznej (łącznie z optymalizacją funkcji kosztu metodą gradientu prostego),\n",
    "* użycie gotowych klas z biblioteki Scikit-learn z zaimplementowanymi gotowymi klasyfikatorami (regresją logistyczną, drzewem decyzyjnym i *k*-NN) i porównanie otrzymanych wyników.\n",
    "\n",
    "\n",
    "# 3. Stanowisko laboratoryjne\n",
    "\n",
    "Do wykonania niniejszego ćwiczenia niezbędne jest stanowisko laboratoryjne, składające się z komputera klasy PC z zainstalowanym oprogramowaniem:\n",
    "* językiem programowania Python (w wersji 3.8),\n",
    "* IDE obsługującym pliki Jupyter Notebook (np. Visual Studio Code z rozszerzeniem ipykernel).\n",
    "\n",
    "\n",
    "# 4. Przebieg ćwiczenia\n",
    "## 4.1. Implementacja algorytmu regresji logistycznej ,,od zera''\n",
    "\n",
    "### Inicjalizacja: import niezbędnych elementów\n",
    "\n",
    "Na początku wykonaj poniższy fragment kodu, aby zaimportować biblioteki niezbędne do wykonania poniższego ćwiczenia:\n",
    "* **Scikit-learn** - biblioteka zawierająca gotowe implementacje wielu algorytmów klasycznego uczenia maszynowego, a także zbiory danych czy metryki. Tutaj skorzystamy ze zbioru danych iris - `datasets.load_iris`.\n",
    "* **NumPy** - biblioteka umożliwiająca wykonywanie wysoko zoptymalizowanych obliczeń matematycznych na objektach typu *numpy array* (wielowymiarowych tablic).\n",
    "* **Matplotlib** - biblioteka wspomagająca wizualizację pracy czy analizę danych poprzez wyświetlanie wykresów."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy==1.22.3 in /home/jakub/py38/lib/python3.8/site-packages (1.22.3)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: scikit-learn==0.24.2 in /home/jakub/py38/lib/python3.8/site-packages (0.24.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/jakub/py38/lib/python3.8/site-packages (from scikit-learn==0.24.2) (3.2.0)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /home/jakub/py38/lib/python3.8/site-packages (from scikit-learn==0.24.2) (1.10.1)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /home/jakub/py38/lib/python3.8/site-packages (from scikit-learn==0.24.2) (1.22.3)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/jakub/py38/lib/python3.8/site-packages (from scikit-learn==0.24.2) (1.3.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: matplotlib==3.4.2 in /home/jakub/py38/lib/python3.8/site-packages (3.4.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/jakub/py38/lib/python3.8/site-packages (from matplotlib==3.4.2) (0.12.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/jakub/py38/lib/python3.8/site-packages (from matplotlib==3.4.2) (10.1.0)\n",
      "Requirement already satisfied: numpy>=1.16 in /home/jakub/py38/lib/python3.8/site-packages (from matplotlib==3.4.2) (1.22.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/jakub/py38/lib/python3.8/site-packages (from matplotlib==3.4.2) (2.8.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/jakub/py38/lib/python3.8/site-packages (from matplotlib==3.4.2) (1.4.5)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /home/jakub/py38/lib/python3.8/site-packages (from matplotlib==3.4.2) (3.1.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/jakub/py38/lib/python3.8/site-packages (from python-dateutil>=2.7->matplotlib==3.4.2) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! python -m pip install numpy==1.22.3\n",
    "! python -m pip install scikit-learn==0.24.2\n",
    "! python -m pip install matplotlib==3.4.2\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Po zaimportowaniu niezbędnych bibliotek, załadujmy gotowy zbiór danych Iris (zawierający dane o wymiarach różnych rodzajów irysów), pochodzący z repozytorium UCI (więcej informacji o tym zbiorze danych możesz uzyskać [TUTAJ](https://archive.ics.uci.edu/dataset/53/iris)). Dane zapiszmy pod zmienną X, a odpowiadające im etykiety - y.\n",
    "\n",
    "Ponadto, dla zobrazowania danych, które będziemy używać, wyświetlmy rozmiary tablic X i y, a także 5 pierszwych rekordów/etykiet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wymiary danych wejściowych: (100, 4)\n",
      "Przykładowe dane wejściowe: \n",
      "[[5.1 3.5 1.4 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.  3.6 1.4 0.2]]\n",
      "Wymiary etykiet: (100,)\n",
      "Przykładowe etykiety: \n",
      "[0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "iris = load_iris()\n",
    "X = iris.data[0:100,:]\n",
    "y = iris.target[0:100]\n",
    "print(\"Wymiary danych wejściowych: \" + str(X.shape))\n",
    "print(\"Przykładowe dane wejściowe: \")\n",
    "print(X[0:5,:])\n",
    "print(\"Wymiary etykiet: \" + str(y.shape))\n",
    "print(\"Przykładowe etykiety: \")\n",
    "print(y[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['setosa' 'versicolor' 'virginica']\n"
     ]
    }
   ],
   "source": [
    "iris2 = load_iris()\n",
    "print(iris2['target_names'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Przyjrzyjmy się wyświetlonym informacjom na temat wczytanych danych. Wczytujemy 100 rekordów o 4 cechach (wymiary tablicy X to 100x4). Etykiety zapisane są w formie 1-wymiarowego wektora y (o wymiarze 100) - z dokumentacji możemy się dowiedzieć, że mogą one przyjmować jedną z 3 wartości: 0, 1 lub 2, symbolizujących przynależność rekordu do jednej z 3 klas (każda klasa oznacza inny typ irysa), lecz w naszym przypadku, aby skupić się jedynie na zagadnieniu binarnej klasyfikacji, podczas ładowania danych odrzucamy ostatnich 50 rekordów należących do 3 klasy, więc operujemy jedynie na 2 klasach.\n",
    "\n",
    "<font size=\"2\">Informacje o wymiarach otrzymanych tablic będą nam bardzo potrzebne na późniejszym etapie, podczas pracy na macierzach przy implementacji procesu terningu i predykcji naszego algorytmu.</font>\n",
    "\n",
    "\n",
    "### Przygotowanie danych\n",
    "\n",
    "Aby móc w pełni korzystać z tych danych, musimy je jednak nieco przekształcić. W tym celu wykonamy dwie operacje:\n",
    "* dokonamy **standaryzacji** danych, tj. przekształcimy je tak, aby bez zmiany ich struktury, każda z cech posiadała średnią o wartości 0 i wariancję o wartości 1 - poprawi to działanie algorytmów uczenia maszynowego, zwłaszcza podczas pracy nad danymi o szerokiej dynamice (różnych rzędach wielkości) czy różnych jednostkach,\n",
    "* podzielimy cały dostępny zbiór danych na 2 zestawy: \n",
    "    * **treningowy** - na podstawie którego model zostanie wytrenowany (to pod te dane zostaną dopasowane parametry naszego modelu),\n",
    "    * **testowy** - który posłuży nam do określenia, jak dobrze działa nasz model na danych, których wcześniej model nie widział.\n",
    "\n",
    "<font size=\"2\">W tym konkretnym przypadku nie będziemy jeszcze wydzielać trzeciego z zazwyczaj tworzonych zestawów danych, zestawu **walidacyjnego**, na podstawie którego dopasowuje się hiperparametry modelu zanim przejdzie się do jego właściwego uczenia - nie ma takiej potrzeby, gdyż nie będziemy w ramach tego ćwiczenia zajmować się dopasowaniem żadnych hiperparametrów.</font>\n",
    "\n",
    "Zacznijmy od napisania funkcji `standarize_data` służącej do normalizacji danych. Zgodnie z poniższym wzorem, zaimplementuj funkcję, która przyjmuje jako argument wejściowe (tablicę X), oblicza średnią $\\mu_n$ i odchylenie standardowe $\\sigma_n$ każdej z cech $x_n$, a następnie odejmuje od nich wyliczone średnie i dzieli je przez odchylenia stardardowe: \n",
    "\\begin{equation*}\n",
    "\tx_n = \\frac{x_n - \\mu_n}{\\sigma_n}\n",
    "\\end{equation*}\n",
    "po czym zwraca tak znormalizowane dane jako tablicę Xnorm.\n",
    "\n",
    "<font size=\"2\">Wskazówki przydatne przy implementacji funkcji `standarize_data`:\n",
    "* Zapoznaj się z dokumentacją dwóch funkcji z biblioteki NumPy: `np.mean` ([TUTAJ](https://numpy.org/doc/stable/reference/generated/numpy.mean.html)) oraz `np.std` ([TUTAJ](https://numpy.org/doc/stable/reference/generated/numpy.std.html)). Zwróć uwagę na parametr `axis`.\n",
    "* Zastosuj mechanizm broadcastigu występujący podczas operacji na tablicach z użyciem NumPy (więcej o tym możesz przeczytać [TUTAJ](https://numpy.org/doc/stable/user/basics.broadcasting.html)).\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.63848179, 0.47633916, 1.44228257, 0.56232019])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.average(X, axis=0)\n",
    "np.std(X, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standarize_data(X):\n",
    "    \"\"\"Funkcja realizująca standaryzację danych: przyrównanie średniej każdej z cech do 0, \n",
    "    a wariancji do 1.\n",
    "\n",
    "    Argument: \n",
    "        - X - nieprzekształcone dane (numpy array, shape = (num_samples, num_features) ). \\n\n",
    "\n",
    "    Zwraca:\n",
    "        - Xnorm - znormalizowane dane (numpy array, shape = (num_samples, num_features) ), \\n\n",
    "        - mu - wektor ze średnimi każdej z cech (numpy array, shape = (num_features,) ), \\n\n",
    "        - sigma - wektor z odchyleniami standardowymi każdej z cech (numpy array, shape = (num_features,) ).\"\"\"\n",
    "\n",
    "    # ------- UZUPEŁNIJ KOD --------\n",
    "    # Oblicz mu - średnią każdej z cech (pamiętaj, że cechy przechowywane są w każdej kolummnie tablicy X)\n",
    "    mu = np.average(X, axis=0)\n",
    "    # Oblicz sigma - odchylenie standardowe każdej z cech\n",
    "    sigma = np.std(X, axis=0)\n",
    "    # Oblicz Xnorm - odejmij średnią od każdej z cech i podziel ją przez jej odchylenie standardowe \n",
    "    # (możesz to zrobić w 1 linijce kodu)\n",
    "    Xnorm = (np.array(X) - mu) / sigma\n",
    "    # -------------------------------\n",
    "    return Xnorm, mu, sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-5.81065904e-01,  8.41837140e-01, -1.01297765e+00,\n",
       "         -1.04211089e+00],\n",
       "        [-8.94308978e-01, -2.07835104e-01, -1.01297765e+00,\n",
       "         -1.04211089e+00],\n",
       "        [-1.20755205e+00,  2.12033793e-01, -1.08231219e+00,\n",
       "         -1.04211089e+00],\n",
       "        [-1.36417359e+00,  2.09934449e-03, -9.43643106e-01,\n",
       "         -1.04211089e+00],\n",
       "        [-7.37687441e-01,  1.05177159e+00, -1.01297765e+00,\n",
       "         -1.04211089e+00],\n",
       "        [-1.11201292e-01,  1.68157493e+00, -8.04974023e-01,\n",
       "         -6.86441647e-01],\n",
       "        [-1.36417359e+00,  6.31902691e-01, -1.01297765e+00,\n",
       "         -8.64276271e-01],\n",
       "        [-7.37687441e-01,  6.31902691e-01, -9.43643106e-01,\n",
       "         -1.04211089e+00],\n",
       "        [-1.67741667e+00, -4.17769553e-01, -1.01297765e+00,\n",
       "         -1.04211089e+00],\n",
       "        [-8.94308978e-01,  2.09934449e-03, -9.43643106e-01,\n",
       "         -1.21994552e+00],\n",
       "        [-1.11201292e-01,  1.26170604e+00, -9.43643106e-01,\n",
       "         -1.04211089e+00],\n",
       "        [-1.05093052e+00,  6.31902691e-01, -8.74308565e-01,\n",
       "         -1.04211089e+00],\n",
       "        [-1.05093052e+00, -2.07835104e-01, -1.01297765e+00,\n",
       "         -1.21994552e+00],\n",
       "        [-1.83403820e+00, -2.07835104e-01, -1.22098127e+00,\n",
       "         -1.21994552e+00],\n",
       "        [ 5.15284858e-01,  1.89150938e+00, -1.15164673e+00,\n",
       "         -1.04211089e+00],\n",
       "        [ 3.58663321e-01,  2.73124718e+00, -9.43643106e-01,\n",
       "         -6.86441647e-01],\n",
       "        [-1.11201292e-01,  1.68157493e+00, -1.08231219e+00,\n",
       "         -6.86441647e-01],\n",
       "        [-5.81065904e-01,  8.41837140e-01, -1.01297765e+00,\n",
       "         -8.64276271e-01],\n",
       "        [ 3.58663321e-01,  1.47164049e+00, -8.04974023e-01,\n",
       "         -8.64276271e-01],\n",
       "        [-5.81065904e-01,  1.47164049e+00, -9.43643106e-01,\n",
       "         -8.64276271e-01],\n",
       "        [-1.11201292e-01,  6.31902691e-01, -8.04974023e-01,\n",
       "         -1.04211089e+00],\n",
       "        [-5.81065904e-01,  1.26170604e+00, -9.43643106e-01,\n",
       "         -6.86441647e-01],\n",
       "        [-1.36417359e+00,  1.05177159e+00, -1.29031581e+00,\n",
       "         -1.04211089e+00],\n",
       "        [-5.81065904e-01,  4.21968242e-01, -8.04974023e-01,\n",
       "         -5.08607024e-01],\n",
       "        [-1.05093052e+00,  6.31902691e-01, -6.66304941e-01,\n",
       "         -1.04211089e+00],\n",
       "        [-7.37687441e-01, -2.07835104e-01, -8.74308565e-01,\n",
       "         -1.04211089e+00],\n",
       "        [-7.37687441e-01,  6.31902691e-01, -8.74308565e-01,\n",
       "         -6.86441647e-01],\n",
       "        [-4.24444366e-01,  8.41837140e-01, -9.43643106e-01,\n",
       "         -1.04211089e+00],\n",
       "        [-4.24444366e-01,  6.31902691e-01, -1.01297765e+00,\n",
       "         -1.04211089e+00],\n",
       "        [-1.20755205e+00,  2.12033793e-01, -8.74308565e-01,\n",
       "         -1.04211089e+00],\n",
       "        [-1.05093052e+00,  2.09934449e-03, -8.74308565e-01,\n",
       "         -1.04211089e+00],\n",
       "        [-1.11201292e-01,  6.31902691e-01, -9.43643106e-01,\n",
       "         -6.86441647e-01],\n",
       "        [-4.24444366e-01,  2.10144383e+00, -9.43643106e-01,\n",
       "         -1.21994552e+00],\n",
       "        [ 4.54202458e-02,  2.31137828e+00, -1.01297765e+00,\n",
       "         -1.04211089e+00],\n",
       "        [-8.94308978e-01,  2.09934449e-03, -9.43643106e-01,\n",
       "         -1.04211089e+00],\n",
       "        [-7.37687441e-01,  2.12033793e-01, -1.15164673e+00,\n",
       "         -1.04211089e+00],\n",
       "        [ 4.54202458e-02,  8.41837140e-01, -1.08231219e+00,\n",
       "         -1.04211089e+00],\n",
       "        [-8.94308978e-01,  1.05177159e+00, -1.01297765e+00,\n",
       "         -1.21994552e+00],\n",
       "        [-1.67741667e+00, -2.07835104e-01, -1.08231219e+00,\n",
       "         -1.04211089e+00],\n",
       "        [-5.81065904e-01,  6.31902691e-01, -9.43643106e-01,\n",
       "         -1.04211089e+00],\n",
       "        [-7.37687441e-01,  8.41837140e-01, -1.08231219e+00,\n",
       "         -8.64276271e-01],\n",
       "        [-1.52079513e+00, -1.67737625e+00, -1.08231219e+00,\n",
       "         -8.64276271e-01],\n",
       "        [-1.67741667e+00,  2.12033793e-01, -1.08231219e+00,\n",
       "         -1.04211089e+00],\n",
       "        [-7.37687441e-01,  8.41837140e-01, -8.74308565e-01,\n",
       "         -3.30772400e-01],\n",
       "        [-5.81065904e-01,  1.47164049e+00, -6.66304941e-01,\n",
       "         -6.86441647e-01],\n",
       "        [-1.05093052e+00, -2.07835104e-01, -1.01297765e+00,\n",
       "         -8.64276271e-01],\n",
       "        [-5.81065904e-01,  1.47164049e+00, -8.74308565e-01,\n",
       "         -1.04211089e+00],\n",
       "        [-1.36417359e+00,  2.12033793e-01, -1.01297765e+00,\n",
       "         -1.04211089e+00],\n",
       "        [-2.67822829e-01,  1.26170604e+00, -9.43643106e-01,\n",
       "         -1.04211089e+00],\n",
       "        [-7.37687441e-01,  4.21968242e-01, -1.01297765e+00,\n",
       "         -1.04211089e+00],\n",
       "        [ 2.39474331e+00,  2.12033793e-01,  1.27506221e+00,\n",
       "          1.09190459e+00],\n",
       "        [ 1.45501408e+00,  2.12033793e-01,  1.13639313e+00,\n",
       "          1.26973921e+00],\n",
       "        [ 2.23812177e+00,  2.09934449e-03,  1.41373130e+00,\n",
       "          1.26973921e+00],\n",
       "        [ 4.54202458e-02, -1.67737625e+00,  7.89720424e-01,\n",
       "          9.14069966e-01],\n",
       "        [ 1.61163562e+00, -6.27704002e-01,  1.20572767e+00,\n",
       "          1.26973921e+00],\n",
       "        [ 3.58663321e-01, -6.27704002e-01,  1.13639313e+00,\n",
       "          9.14069966e-01],\n",
       "        [ 1.29839254e+00,  4.21968242e-01,  1.27506221e+00,\n",
       "          1.44757384e+00],\n",
       "        [-8.94308978e-01, -1.46744180e+00,  3.04378636e-01,\n",
       "          3.80566095e-01],\n",
       "        [ 1.76825716e+00, -4.17769553e-01,  1.20572767e+00,\n",
       "          9.14069966e-01],\n",
       "        [-4.24444366e-01, -8.37638451e-01,  7.20385883e-01,\n",
       "          1.09190459e+00],\n",
       "        [-7.37687441e-01, -2.30717959e+00,  4.43047718e-01,\n",
       "          3.80566095e-01],\n",
       "        [ 6.71906395e-01, -2.07835104e-01,  9.28389507e-01,\n",
       "          1.26973921e+00],\n",
       "        [ 8.28527933e-01, -1.88731069e+00,  7.89720424e-01,\n",
       "          3.80566095e-01],\n",
       "        [ 9.85149470e-01, -4.17769553e-01,  1.27506221e+00,\n",
       "          1.09190459e+00],\n",
       "        [ 2.02041783e-01, -4.17769553e-01,  5.12382260e-01,\n",
       "          9.14069966e-01],\n",
       "        [ 1.92487869e+00,  2.09934449e-03,  1.06705859e+00,\n",
       "          1.09190459e+00],\n",
       "        [ 2.02041783e-01, -2.07835104e-01,  1.13639313e+00,\n",
       "          1.26973921e+00],\n",
       "        [ 5.15284858e-01, -8.37638451e-01,  8.59054966e-01,\n",
       "          3.80566095e-01],\n",
       "        [ 1.14177101e+00, -1.88731069e+00,  1.13639313e+00,\n",
       "          1.26973921e+00],\n",
       "        [ 2.02041783e-01, -1.25750735e+00,  7.20385883e-01,\n",
       "          5.58400718e-01],\n",
       "        [ 6.71906395e-01,  2.12033793e-01,  1.34439675e+00,\n",
       "          1.80324308e+00],\n",
       "        [ 9.85149470e-01, -6.27704002e-01,  7.89720424e-01,\n",
       "          9.14069966e-01],\n",
       "        [ 1.29839254e+00, -1.25750735e+00,  1.41373130e+00,\n",
       "          1.26973921e+00],\n",
       "        [ 9.85149470e-01, -6.27704002e-01,  1.27506221e+00,\n",
       "          7.36235342e-01],\n",
       "        [ 1.45501408e+00, -4.17769553e-01,  9.97724048e-01,\n",
       "          9.14069966e-01],\n",
       "        [ 1.76825716e+00, -2.07835104e-01,  1.06705859e+00,\n",
       "          1.09190459e+00],\n",
       "        [ 2.08150023e+00, -6.27704002e-01,  1.34439675e+00,\n",
       "          1.09190459e+00],\n",
       "        [ 1.92487869e+00, -2.07835104e-01,  1.48306584e+00,\n",
       "          1.62540846e+00],\n",
       "        [ 8.28527933e-01, -4.17769553e-01,  1.13639313e+00,\n",
       "          1.26973921e+00],\n",
       "        [ 3.58663321e-01, -1.04757290e+00,  4.43047718e-01,\n",
       "          3.80566095e-01],\n",
       "        [ 4.54202458e-02, -1.46744180e+00,  6.51051342e-01,\n",
       "          5.58400718e-01],\n",
       "        [ 4.54202458e-02, -1.46744180e+00,  5.81716801e-01,\n",
       "          3.80566095e-01],\n",
       "        [ 5.15284858e-01, -8.37638451e-01,  7.20385883e-01,\n",
       "          7.36235342e-01],\n",
       "        [ 8.28527933e-01, -8.37638451e-01,  1.55240038e+00,\n",
       "          1.44757384e+00],\n",
       "        [-1.11201292e-01, -2.07835104e-01,  1.13639313e+00,\n",
       "          1.26973921e+00],\n",
       "        [ 8.28527933e-01,  6.31902691e-01,  1.13639313e+00,\n",
       "          1.44757384e+00],\n",
       "        [ 1.92487869e+00,  2.09934449e-03,  1.27506221e+00,\n",
       "          1.26973921e+00],\n",
       "        [ 1.29839254e+00, -1.67737625e+00,  1.06705859e+00,\n",
       "          9.14069966e-01],\n",
       "        [ 2.02041783e-01, -2.07835104e-01,  8.59054966e-01,\n",
       "          9.14069966e-01],\n",
       "        [ 4.54202458e-02, -1.25750735e+00,  7.89720424e-01,\n",
       "          9.14069966e-01],\n",
       "        [ 4.54202458e-02, -1.04757290e+00,  1.06705859e+00,\n",
       "          7.36235342e-01],\n",
       "        [ 9.85149470e-01, -2.07835104e-01,  1.20572767e+00,\n",
       "          1.09190459e+00],\n",
       "        [ 5.15284858e-01, -1.04757290e+00,  7.89720424e-01,\n",
       "          7.36235342e-01],\n",
       "        [-7.37687441e-01, -1.67737625e+00,  3.04378636e-01,\n",
       "          3.80566095e-01],\n",
       "        [ 2.02041783e-01, -8.37638451e-01,  9.28389507e-01,\n",
       "          9.14069966e-01],\n",
       "        [ 3.58663321e-01, -2.07835104e-01,  9.28389507e-01,\n",
       "          7.36235342e-01],\n",
       "        [ 3.58663321e-01, -4.17769553e-01,  9.28389507e-01,\n",
       "          9.14069966e-01],\n",
       "        [ 1.14177101e+00, -4.17769553e-01,  9.97724048e-01,\n",
       "          9.14069966e-01],\n",
       "        [-5.81065904e-01, -1.25750735e+00,  9.63750123e-02,\n",
       "          5.58400718e-01],\n",
       "        [ 3.58663321e-01, -6.27704002e-01,  8.59054966e-01,\n",
       "          9.14069966e-01]]),\n",
       " array([5.471, 3.099, 2.861, 0.786]),\n",
       " array([0.63848179, 0.47633916, 1.44228257, 0.56232019]))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "standarize_data(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teraz przejdźmy do utworzenia funkcji `split_data`, która ma za zadanie podzielić dostarczony jej zbiór danych X (i etykiety y) na zestaw treningowy i testowy, zgodnie z podanym jako argument procentem (wartość `percentage_train` odpowiadać ma procentowi, jaką częścią oryginalnego zbioru danych ma być zbiór treningowy).\n",
    "\n",
    "<font size=\"2\">Wskazówki przydatne przy implementacji funkcji `split_data`:\n",
    "* Zapoznaj się z dokumentacją funkcji `np.random.choice` z biblioteki NumPy ([TUTAJ](https://numpy.org/doc/stable/reference/random/generated/numpy.random.choice.html)). Zwróć uwagę na parametr `replace`.\n",
    "* Zapoznaj się z trickiem na usunięcie elementów jednej listy z drugiej z wykorzystaniem różnicy zbiorów (więcej o tym możesz przeczytać [TUTAJ](https://stackoverflow.com/questions/3428536/how-do-i-subtract-one-list-from-another/)). Do utworzenia listy indeksów wszystkich rekordów danych możesz użyć funkcji `np.arange` z biblioteki NumPy ([TUTAJ](https://numpy.org/doc/stable/reference/generated/numpy.arange.html)).\n",
    "* Zapoznaj się ze sposobami na wyodrębnienie części tablicy w NumPy ([TUTAJ](https://numpy.org/doc/stable/user/basics.indexing.html)). Możesz je filtrować według elementów innej listy!\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(X, y, percentage_train = 70.0, seed=100):\n",
    "    \"\"\"Funkcja dzieląca losowo dane na zestaw treningowy oraz testowy w zadanej proporcji. \\n\n",
    "    \n",
    "    Argumenty: \\n\n",
    "      - X - dane wejściowe (numpy array, shape = (num_samples, num_features) ), \\n\n",
    "      - y - etykiety (numpy array, shape = (num_samples,) ), \\n\n",
    "      - percentage_train (argument opcjonalny) - jakim procentem wejściowych danych ma być \n",
    "        zestaw treningowy (skalar, float, domyślna wartość: 70). \\n\n",
    "    Zwraca: \\n\n",
    "      - Xtrain - dane treningowe (numpy array, shape = (num_samples * percentage_train, num_features) ), \\n\n",
    "      - ytrain - etykiety do danych treningowych (numpy array, shape = (num_samples * percentage_train,) ), \\n\n",
    "      - Xtest - dane testowe (numpy array, shape = (num_samples * (100-percentage_train), num_features) ), \\n\n",
    "      - ytest - etykiety do danych testowych (numpy array, shape = (num_samples * (100-percentage_train),) ).\"\"\"\n",
    "    \n",
    "    np.random.seed(seed) # Dla zapanowania nad \"losowością\"\n",
    "    num_all_datapoints =  X.shape[0] # Ilość wszystkich danych\n",
    "    num_train_datapoints = int(np.round(X.shape[0]*percentage_train/100)) # Docelowa wielkość zestawu treningowego\n",
    "    \n",
    "    # ------- UZUPEŁNIJ KOD --------\n",
    "    # Znając rozmiar danych wejściowych, wygeneruj indices_train - listę indeksów tych rekordów, \n",
    "    # które mają należeć do zestawu treningowego\n",
    "    indices_train = np.random.choice(X.shape[0], size=num_train_datapoints, replace=False)\n",
    "    ind = np.zeros(X.shape[0], dtype=bool) # https://stackoverflow.com/questions/50491630/randomly-split-a-numpy-array\n",
    "    ind[indices_train] = True\n",
    "    indices_test = ~ind\n",
    "    # Wygeneruj zmienne z właściwie podzielonym zbiorem danych: Xtrain, ytrain, Xtest, ytest\n",
    "    Xtrain = X[indices_train]\n",
    "    ytrain = y[indices_train]\n",
    "    Xtest = X[indices_test]\n",
    "    ytest = y[indices_test]\n",
    "    # ------------------------------\n",
    "    \n",
    "    return Xtrain, ytrain, Xtest, ytest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pora sprawdzić, jak działają napisane przez Ciebie funkcje! Uruchom poniższy kod, aby podzielić nasz zbiór danych na zestaw treningowy (powinien domyślnie zawierać 70 elementów) i testowy (pozostałe 30 elementów). Elementy w poszczególnych cechach powinny po normalizacji oscylować wokół 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wymiary danych treningowych: (70, 4)\n",
      "Wymiary danych testowych: (30, 4)\n",
      "Przykładowe znormalizowane dane treningowe: \n",
      "[[-0.97023354  1.12689892 -1.11317579 -1.32513741]\n",
      " [ 0.73542816 -1.9493154   0.67340264  0.2589927 ]\n",
      " [-0.81517338  0.68743973 -0.97574668 -0.79709404]\n",
      " [-1.59047416 -1.7295858  -1.18189035 -0.9731085 ]\n",
      " [-0.81517338  0.24798054 -1.2506049  -1.14912295]]\n"
     ]
    }
   ],
   "source": [
    "# Podział danych na część treningową i testową\n",
    "Xtrain, ytrain, Xtest, ytest = split_data(X ,y)\n",
    "print(\"Wymiary danych treningowych: \"+ str(Xtrain.shape))\n",
    "print(\"Wymiary danych testowych: \"+ str(Xtest.shape))\n",
    "\n",
    "# Normalizacja obu zestawów danych\n",
    "Xtrain_norm, _, _ = standarize_data(Xtrain) # pomijamy zwracanie mu i sigma dla danych treningowych\n",
    "Xtest_norm, mu_test, sigma_test = standarize_data(Xtest)\n",
    "print(\"Przykładowe znormalizowane dane treningowe: \")\n",
    "print(Xtrain_norm[0:5,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trening modelu regresji logistycznej\n",
    "\n",
    "Teraz, kiedy mamy już gotowe dane, na których możemy pracować, przejdźmy do najważniejszej rzeczy, czyli napisania funkcji, które utworzą nasz model oparty na regresji logistycznej i pozwolą mu się uczyć!\n",
    "\n",
    "Przypomnijmy, że **hipotezą** $h_\\theta(x)$ (tj. funkcją, która wiąże parametry modelu i dane wejściowe, dając w wyniku predykcje) regresji logistycznej jest: \n",
    "\\begin{equation*}\n",
    "    h_\\theta(x) = g(\\theta^Tx)\n",
    "\\end{equation*}\n",
    "gdzie funkcja $g(z)$ jest to tzw. funkcja **sigmoid** o następującej postaci:\n",
    "\\begin{equation*}\n",
    "    g(z) = \\frac{1}{1+e^{-z}}\n",
    "\\end{equation*}\n",
    "\n",
    "<div align=\"center\">\n",
    "\n",
    "<img src='https://miro.medium.com/v2/resize:fit:1280/1*OUOB_YF41M-O4GgZH_F2rw.png' />\n",
    "\n",
    "<font size=\"1\">Grafika: towardsdatascience.com</font>\n",
    "</div>\n",
    "\n",
    "Funkcja sigmoid ma kilka ciekawych właściwości, dzięki którym jest często spotykana przy rozwiązywaniu problemów klasyfikacji: jej wartości zawierają się w przedziale od 0 do 1 (dlatego można je traktować jak prawdopodobieństwo należenia danego rekordu danych do pewnej klasy i np. traktować te dane, dla których sigmoid zwrócił wartość większą niż 0,5, jako należące do tejże klasy), a także jest odwracalna i różniczkowalna, dzięki czemu da się obliczać jej gradient niezbędny w procesie uczenia modelu. Zaimplementuj zatem funkcję `sigmoid`, która zwraca wartość sigmoidu dla dowolnej *numpy array*!\n",
    "\n",
    "<font size=\"2\">Wskazówka przydatna przy implementacji funkcji `sigmoid`: Warto skorzystać z funkcji `np.exp` z biblioteki NumPy ([TUTAJ](https://numpy.org/doc/stable/reference/generated/numpy.exp.html))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"Funkcja obliczająca wartość sigmoidu dla zadanego argumentu z. \\n\n",
    "    \"\"\"\n",
    "    # ------- UZUPEŁNIJ KOD --------\n",
    "    sigmoid = 1/(1 + np.exp(-z)) \n",
    "    # ------------------------------\n",
    "    return sigmoid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sprawdź poprawność implementacji funkcji `sigmoid`. Obliczymy, co zwraca ta funkcja, gdy argumentem są same zera: skalar, wektor 1-D oraz macierz 2-D. W każdym przypadku, sigmoid powinien zwrócić stukturę o takich samych wymiarach, jak dane wejściowe, a każdy z jej elementów powinien wynosić 0,5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sigmoid dla skalara: 0.5\n",
      "Sigmoid dla wektora: [0.5 0.5 0.5]\n",
      "Sigmoid dla macierzy: [[0.5 0.5 0.5]\n",
      " [0.5 0.5 0.5]\n",
      " [0.5 0.5 0.5]]\n"
     ]
    }
   ],
   "source": [
    "# Testowanie poprawności implementacji sigmoidu\n",
    "print(\"Sigmoid dla skalara: \" + str(sigmoid(0)))\n",
    "print(\"Sigmoid dla wektora: \" + str(sigmoid(np.zeros((3)))))\n",
    "print(\"Sigmoid dla macierzy: \" + str(sigmoid(np.zeros((3,3)))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jako funkcję kosztu $J(\\theta)$ użyjemy **binarnej entropii krzyżowej** (ang. *Binary Cross Entropy*, BCE), często spotykaną przy okazji problemów klasyfikacji binarnej. Jej wartość jest tym większa, im więcej pomyłek popełni klasyfikator: przy zgodności etykiety $y^{(i)}$ i predykcji $h_\\theta(x^{(i)})$, oba człony wyrażenia zerują się. Funkcja ta opisana jest wzorem:\n",
    "\\begin{equation*}\n",
    "\tJ(\\theta) = \\frac{1}{m} \\sum_{i=1}^{m} [-y^{(i)}\\log(h_\\theta(x^{(i)})) - (1-y^{(i)})\\log(1-h_\\theta(x^{(i)}))]\t\n",
    "\\end{equation*}\n",
    "\n",
    "Jak już wiesz, trening modelu opiera się na znalezieniu optymalnych parametrów $\\theta$, tj. takich, przy których funkcja kosztu jest minimalna. My taką optymalizację przeprowadzimy z wykorzystaniem metody gradientu prostego, która do poprawnego działania musi znać gradient $\\frac{\\partial J(\\theta)}{\\partial \\theta}$ naszej funcji kosztu. Pamiętaj, że **gradient ma taki sam wymiar, jak wektor parametrów $\\theta$**, a zatem składa się z $n$ elementów. W przypadku gradientu fukcji BCE, każdy z jego $n$ elementów można obliczyć z następującego wzoru (pomijam tutaj jego wyprowadzenie):\n",
    "\\begin{equation*}\n",
    "\t\\frac{\\partial J(\\theta)}{\\partial \\theta_n} = \\frac{1}{m} \\sum_{i=1}^{m} (h_\\theta(x^{(i)})-y^{(i)}) \\cdot x^{(i)}_n\n",
    "\\end{equation*}\n",
    "\n",
    "Napisz zatem funkcję `compute_cost_and_gradient`, w której na podstawie danych treningowych i zadanych parametrów $\\theta$, obliczysz koszt BCE i jego gradient.\n",
    "\n",
    "<font size=\"2\">Wskazówki przydatne przy implementacji funkcji `compute_cost_and_gradient`:\n",
    "* Przy obliczaniu kosztu, będzie Ci na pewno potrzebna funkcja `np.log2` z biblioteki NumPy ([TUTAJ](https://numpy.org/doc/stable/reference/generated/numpy.log2.html)) oraz napisana przez Ciebie wcześniej funkcja `sigmoid`.\n",
    "* Zamiast używania pętli `for` do iteracji po wszystkich $m$ elementach, możesz wykonać operacje na macierzach (z wykorzystaniem funkcji `np.dot` ([TUTAJ](https://numpy.org/doc/stable/reference/generated/numpy.dot.html))). Pamiętaj jednak o właściwościach mnożenia macierzy - nie każde macierze da się przemnożyć, muszą one mieć zgodne \"wewnętrze\" wymiary (ilość kolumn pierwszej macierzy musi być taka sama, jak ilość wierszy drugiej macierzy; wówczas w wyniku mnożenia otrzymujemy macierz o zgodnych \"zewnętrznych\" wymiarach, tj. o liczbie wierszy jak pierwsza macierz i liczbie kolumn jak druga macierz: [M,N]x[N,1]=[M,1]): w razie potrzeby zmień kolejność mnożonych macierzy albo dokonaj ich transpozycji z użyciem funkcji `np.transpose` ([TUTAJ](https://numpy.org/doc/stable/reference/generated/numpy.transpose.html)).\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost_and_gradient(X, y, theta):\n",
    "    \"\"\"Funkcja obliczająca koszt BCE dla regresji logistycznej. \\n\n",
    "    Argumenty: \\n\n",
    "    X - dane wejściowe (numpy array, shape = (num_samples, num_features) ), \\n\n",
    "    y - etykiety (numpy array, shape = (num_samples,) ), \\n\n",
    "    theta - zestaw parametrów (numpy array, shape = (num_features,) ). \\n\n",
    "    Zwraca: \\n\n",
    "    J - obliczony koszt (skalar, float), \\n\n",
    "    grad - obliczony gradient funkcji kosztu (numpy array, shape=(num_features) ). \"\"\"\n",
    "    \n",
    "    # ------- UZUPEŁNIJ KOD --------\n",
    "    # Oblicz wartość funkcji kosztu\n",
    "     \n",
    "\n",
    "   \n",
    "\n",
    "    # Oblicz wartość funkcji kosztu\n",
    "    m = X.shape[0]\n",
    "    predictions = sigmoid(np.dot(X, theta))\n",
    "    J = (-1/m) * np.sum(y * np.log(predictions) + (1 - y) * np.log(1 - predictions))\n",
    "\n",
    "    # Oblicz gradient\n",
    "    grad = (1/m) * np.dot(X.T, (predictions - y))\n",
    "\n",
    "    return J, grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sprawdźmy poprawność implementacji powyższej funkcji: uruchom następującą komórkę, aby wyliczyć przykładowe wartości kosztu i jego gradientu dla zerowych parametrów (kiedy wszystkie elementy $\\theta$ są równe 0), liczone dla całego naszego zbioru danych. Koszt powinien wynosić 1, a gradient [-0.2325,  0.1645, -0.6995, -0.27]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Przykładowy koszt i jego gradient, liczony dla zerowych parmetrów: (0.6931471805599453, array([-0.2325,  0.1645, -0.6995, -0.27  ]))\n"
     ]
    }
   ],
   "source": [
    "# Testowanie poprawności implementacji funkcji kosztu\n",
    "print(\"Przykładowy koszt i jego gradient, liczony dla zerowych parmetrów: \"+str(compute_cost_and_gradient(\n",
    "    X,y,np.zeros((X.shape[1])))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mając wyznaczoną funkcję kosztu, możemy przejść do jej optymalizacji, czyli doboru takich wartości parametrów $\\theta$, które dają najniższą wartość kosztu (tj. najlepiej odwzorowują dane treningowe). Jak już wspomniano, zrobimy to z wykorzystaniem metody gradientu prostego, według której aktualizacja parametrów odbywa się według poniższego wzoru:\n",
    "\\begin{equation*}\n",
    "    \\theta_n := \\theta_n - \\alpha \\cdot \\frac{\\partial J(\\theta)}{\\partial \\theta_n}\n",
    "\\end{equation*}\n",
    "gdzie hiperparametr $\\alpha$ (którego wartość musi być zdefiniowana zanim przejdzie się do treningu modelu), oznacza, jak bardzo gradient funkcji kosztu ma wpływ na nową, zaktualizowaną postać parametrów $\\theta$\n",
    "\n",
    "Napisz zatem funkcję `train_logistic_regression`, w której na podstawie danych treningowych, iteracyjnie liczona jest wartość funkcji kosztu i jego gradient, parametry są aktualizowane zgodnie z metodą gradientu prostego, a ponadto przy każdej itaracji wizualizowana jest nowa wartość kosztu, aby móc ocenić, czy w ramach treningu koszt rzeczywiście spada (ważne - jeśli zaobserwowalibyśmy wzrost kosztu wraz z kolejnymi  iteracjami, oznacza to, że model CORAZ GORZEJ radzi sobie z analizą danych treningowych, a zatem wcale się nie uczy!). \"Szkielet\" tej funkcji został już napisany, wykonanych zostanie 400 iteracji, a stała uczenia może zostać w postaci domyślnej (ustawionej na 0,01).\n",
    "\n",
    "<font size=\"2\">Poprawność implementacji tej funkcji sprawdzimy nieco później.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_logistic_regression(X,y,alpha=0.01):\n",
    "    \"\"\"Funkcja realizująca optymalizację funkcji kosztu dla regresji logistycznej\n",
    "    w celu wytrenowania modelu (otrzymania zestawu najlepszych parametrów, theta)\n",
    "    z wykorzystaniem metody gradientu prostego. \\n\n",
    "    Argumenty: \\n\n",
    "    X - dane treningowe (numpy array, shape = (num_samples, num_features) ), \\n\n",
    "    y - etykiety (numpy array, shape = (num_samples,) ), \\n\n",
    "    alpha (opcjonalnie) - stała uczenia (skalar, float, domyślnie 0.001). \\n\n",
    "    Zwraca: theta - zestaw optymalnych parametrów (numpy array, shape = (num_features,) ). \"\"\"\n",
    "    \n",
    "    # Inicjalizacja\n",
    "    num_iterations = 400 # tyle razy wykonać ma się gradient descent\n",
    "    theta = np.zeros((X.shape[1])) # wstępna inicjalizacja parametrów samymi zerami\n",
    "    Js = np.zeros(num_iterations) # wektor przechowujący dotychczasowe wartości kosztu (do wizualizacji)\n",
    "    \n",
    "    # Uruchomienie metody gradientów prostych\n",
    "    print(\"\\nTrwa trening modelu... \")\n",
    "    for i in range(num_iterations):\n",
    "         # Calculate the cost and gradient using your compute_cost_and_gradient function\n",
    "        J, grad = compute_cost_and_gradient(X, y, theta)\n",
    "        Js[i] = J  # Store the cost in the Js array\n",
    "        \n",
    "        # Update theta using gradient descent\n",
    "        theta -= alpha * grad\n",
    "    print(\"Zakończono. \")\n",
    "    \n",
    "    # Wizualizacja zmian kosztu\n",
    "    plt.figure()\n",
    "    plt.plot(Js)\n",
    "    plt.title(\"Efekty treningu modelu - zmiany w koszcie\")\n",
    "    plt.xlabel(\"Numer iteracji\")\n",
    "    plt.ylabel(\"Wartość funkcji kosztu\")\n",
    "    plt.show(block=False)\n",
    "    \n",
    "    return theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predykcja i poskładanie wszystkiego w całość!\n",
    "\n",
    "Wytrenowany model musi umieć dokonywać predykcji - w tym przypadku, podejmować decyzję, czy analizowany rekord danych zaklasyfikować do klasy pierwszej (etykieta 0) czy drugiej (etykieta 1). Napisz zatem ostatnią w tej części ćwiczenia funkcję, `predict_logistic_regression`, która oblicza funkcję hipotezy dla regresji logistycznej i zwraca odpowiednie etykiety.\n",
    "\n",
    "<font size=\"2\">Wskazówka przydatna przy implementacji funkcji `predict_logistic_regression`: Pamiętaj, że model ma zwrócić etykietę 0 w sytuacji, kiedy prawdopodobieństwo obliczone z wykorzystaniem hipotezy jest mniejsze niż 0,5. Poszukaj funkcji z biblioteki NumPy, która realizuje przybliżanie do najbliższej liczby naturalnej!\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_logistic_regression(X,theta):\n",
    "    \"\"\"Funkcja obliczająca ostateczną predykcję regresji logistycznej. \\n\n",
    "    Argumenty: \\n\n",
    "    X - dane wejściowe (numpy array, shape = (num_samples, num_features) ), \\n\n",
    "    theta - zestaw optymalnych parametrów (numpy array, shape = (num_features,) ). \\n\n",
    "    Zwraca: pred - dokonane predykcje (numpy array, shape = (num_samples,) ). \"\"\"\n",
    "    z = np.dot(X, theta)\n",
    "    pred = sigmoid(z)\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sprawdź poprawność implementacji funkcji `predict_logistic_regression`. W poniższej komórce wykonujemu tę funkcję dla pierwszych pięciu rekordów oryginalnego zbioru danych i zerowych parametrów. Powinieneś otrzymać w wyniku same zera."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Przykładowe predykcje przy zerowych parametrach: [0.5 0.5 0.5 0.5 0.5]\n"
     ]
    }
   ],
   "source": [
    "# Testowanie poprawności implementacji predykcji\n",
    "print(\"Przykładowe predykcje przy zerowych parametrach: \"+str(predict_logistic_regression(X[0:5,:],np.zeros((X.shape[1])))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Czas uruchomić całość - od właściwego uczenia naszego modelu, aż po dokonanie przezeń predykcji na danych testowych! Uruchom poniższy kod, w którym wykorzystujemy niemal wszystko, co do tej pory udało nam się napisać. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Trwa trening modelu... \n",
      "Zakończono. \n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dokładność modelu na danych testowych: 0.0%\n"
     ]
    }
   ],
   "source": [
    "# Trening modelu (na danych treningowych)\n",
    "theta = train_logistic_regression(Xtrain_norm,ytrain)\n",
    "# Dokonanie predykcji (na danych testowych) i obliczenie dokładności modelu\n",
    "pred = predict_logistic_regression(Xtest_norm,theta)\n",
    "accuracy = np.mean(pred==ytest)\n",
    "print(\"Dokładność modelu na danych testowych: \"+str(accuracy*100)+'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jeśli na wykresie obserwujesz stopniowy spadek kosztu - świetnie, Twój model się uczy! Można też zaobserwować bardzo wysoką wartość dokładności (**dokładność** to procent decyzji, które model podjął właściwie) naszego modelu na danych z zestawu testowego, świadczącą o jego poprawnym działaniu. \n",
    "\n",
    "\n",
    "## 4.2. Uruchomienie klasyfikatorów z biblioteki Scikit-learn\n",
    "\n",
    "### Import nowych niezbędnych klas\n",
    "\n",
    "Udało Ci się stworzyć model klasyfikatora opartego o regresję logistyczną od zera. Na szczęście nie zawsze trzeba włożyć tyle pracy, aby móc używać algorytmów uczenia maszynowego. Istnieją biblioteki posiadające gotowe implementacje wielu z nich. Jedną z takich bibliotek jest wykorzystywana już przez Ciebie biblioteka Scikit-learn. Tym razem do sprawdzenia, jak z analizowanym tutaj zbiorze danych iris radzą sobie inne algorytmy, wykorzystamy jej gotowe klasy i metody:\n",
    "- model regresji logistycznej - `linear_model.LogisticRegression` (dokumentacja [TUTAJ](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)), \n",
    "- model drzewa decyzyjnego - `tree.DecisionTreeClassifier` ([TUTAJ](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)),\n",
    "- model *k*-NN - `neighbors.KNeighborsClassifier` ([TUTAJ](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html)),\n",
    "- metrykę dokładności - `metrics.accuracy_score` ([TUTAJ](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html)),\n",
    "- klasę standaryzującą dane - `preprocessing.StandardScaler` ([TUTAJ](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)),\n",
    "- funkcję dzielącą dane na zestaw treningowy i testowy - `model_selection.train_test_split` ([TUTAJ](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)).\n",
    "\n",
    "Uruchom poniższą komórkę, aby zaimportować niezbędne klasy i metody. Zapoznaj się też z ich dokumentacją."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Przygotowanie danych z użyciem gotowych metod\n",
    "\n",
    "Mając wciąż w pamięci przechowywane zmienne X i y z oryginalnym, jeszcze nieznormalizowanym ani niepodzielonym zbiorem danych iris (oraz etykietami), możemy je ponownie wykorzystać w tej części ćwiczenia. Napisz więc fragment kodu, który:\n",
    "* dzieli dane X i y na właściwe zestawy Xtrain i Xtest (użyj metody `train_test_split`) - niech zestaw treningowy zawiera 70% danych,\n",
    "* standaryzuje Xtrain i Xtest (zwróć uwagę na metodę `fit_transform` klasy `StandardScaler`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wymiary danych treningowych: (70, 4)\n",
      "Wymiary danych testowych: (30, 4)\n",
      "Przykładowe znormalizowane dane treningowe: \n",
      "[[-1.0280556   0.72748234 -0.92621165 -1.09332559]\n",
      " [-1.32853114  0.29036849 -1.06870575 -1.09332559]\n",
      " [ 0.77479765  0.72748234  1.1399528   1.42775459]\n",
      " [-0.42710452  0.72748234 -1.06870575 -1.09332559]\n",
      " [-0.72758006 -1.67664385  0.2849882   0.34729166]]\n"
     ]
    }
   ],
   "source": [
    "# ------------ UZUPEŁNIJ KOD -------------\n",
    "# Podziel dane z X i y na zestaw treningowy (70%) i testowy (30%) z wykorzystaniem metody train_test_split\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "# Dokonaj normalizacji danych treningowych z wykorzystaniem klasy StandardScaler - zwróć Xtrain_norm\n",
    "scaler = StandardScaler()\n",
    "Xtrain_norm = scaler.fit_transform(Xtrain)\n",
    "# Dokonaj normalizacji danych testowych - zwróć Xtest_norm\n",
    "Xtest_norm = scaler.transform(Xtest)\n",
    "# ----------------------------------------\n",
    "\n",
    "print(\"Wymiary danych treningowych: \" + str(Xtrain.shape))\n",
    "print(\"Wymiary danych testowych: \" + str(Xtest.shape))\n",
    "print(\"Przykładowe znormalizowane dane treningowe: \")\n",
    "print(Xtrain_norm[0:5, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regresja logistyczna z biblioteki Scikit-learn\n",
    "\n",
    "Mamy przygotowane dane, pora zatem wytrenować model regresji logistycznej z wykorzystaniem klasy `LogisticRegression`!\n",
    "* Do trenowania (na bazie zestandaryzowanego zestawu treningowego) użyj metody `fit`.\n",
    "* Do dokonania predykcji użyj metody `predict` - zobacz, jakich predykcji dokona Twój model na widok danych z zestandaryzowanego zestawu testowego (zapisz je do zmiennej o nazwie `pred`).\n",
    "* Oblicz dokładność predykcji Twojego modelu (dla zestandaryzowanych danych testowych) z wykorzystaniem metody `accuracy_score`. Wynik zapisz do zmiennej o nazwie `acccuracy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dokładność modelu na danych testowych: 100.0%\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression() # Leave the default settings\n",
    "\n",
    "# ------------ UZUPEŁNIJ KOD -------------\n",
    "# Wytrenuj model regresji logistycznej na zestandaryzowanych danych treningowych\n",
    "model.fit(Xtrain_norm, ytrain)\n",
    "\n",
    "# Zwróć predykcję modelu (do zmiennej o nazwie pred) dla zestandaryzowanych danych testowych\n",
    "pred = model.predict(Xtest_norm)\n",
    "\n",
    "# Oblicz dokładność predykcji\n",
    "accuracy = accuracy_score(ytest, pred)\n",
    "# ----------------------------------------\n",
    "\n",
    "print(\"Dokładność modelu na danych testowych: \" + str(accuracy * 100) + '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inne klasyfikatory dostępne w Scikit-learn\n",
    "\n",
    "Na sam koniec, sprawdź, jak zachowują się inne modele klasyfikatorów, dostępne w ramach biblioteki Scikit-learn: drzewo decyzyjne oraz *k*-NN! Uruchom po prostu poniższy kod:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dokładność drzewa decyzyjnego na danych testowych: 100.0%\n",
      "Dokładność k-NN na danych testowych: 100.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jakub/py38/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    }
   ],
   "source": [
    "# Drzewo decyzyjne\n",
    "model2 = DecisionTreeClassifier(max_depth=5)\n",
    "model2.fit(Xtrain_norm,ytrain)\n",
    "pred = model2.predict(Xtest_norm)\n",
    "accuracy = accuracy_score(ytest,pred)\n",
    "print(\"Dokładność drzewa decyzyjnego na danych testowych: \"+str(accuracy*100)+'%')\n",
    "\n",
    "# k-NN\n",
    "model3 = KNeighborsClassifier(n_neighbors=3)\n",
    "model3.fit(Xtrain_norm,ytrain)\n",
    "pred = model3.predict(Xtest_norm)\n",
    "accuracy = accuracy_score(ytest,pred)\n",
    "print(\"Dokładność k-NN na danych testowych: \"+str(accuracy*100)+'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gratulacje! W ten sposób zakończyliśmy to ćwiczenie, w którym skupiliśmy się na działaniu klasycznych algorytmów klasyfikacji.\n",
    "\n",
    "\n",
    "## 5. Pytania kontrolne\n",
    "\n",
    "1. Czym różni się uczenie nadzorowane od nienadzorowanego?\n",
    "2. Jakie są różnice pomiędzy problemem regresji a klasyfikacji?\n",
    "3. Opisz krótko, na czym polega trening modelu uczenia maszynowego z wykorzystaniem metody gradientów prostych."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Odp\n",
    "\n",
    "1. Różnica między uczeniem nadzorowanym a nienadzorowanym polega na obecności lub braku etykiet wyjściowych w danych treningowych oraz na różnych celach, jakie te dwa rodzaje uczenia mają. W uczeniu nadzorowanym modele są trenowane do przewidywania lub klasyfikacji na podstawie dostarczonych etykiet, podczas gdy w uczeniu nienadzorowanym celem jest wyodrębnienie ukrytych wzorców lub struktur w danych.\n",
    "\n",
    "2. Różnica między problemem regresji a klasyfikacji polega na rodzaju wyników, które model próbuje przewidzieć. W regresji przewiduje się liczby rzeczywiste, podczas gdy w klasyfikacji przyporządkowuje się dane do dyskretnych kategorii lub klas.\n",
    "\n",
    "3. Trening modelu uczenia maszynowego przy użyciu metody gradientów prostych (Gradient Descent) jest jednym z najważniejszych procesów w uczeniu maszynowym, szczególnie w kontekście uczenia nadzorowanego. \n",
    "\n",
    "- Celem treningu jest dostosowanie modelu do danych treningowych w taki sposób, aby osiągnąć jak najlepsze dopasowanie modelu do rzeczywistych danych. \n",
    "- W procesie treningu określa się funkcję kosztu (ang. cost function), która mierzy, jak dobrze model przewiduje dane treningowe w porównaniu do rzeczywistych etykiet.\n",
    "- Gradient to wektor pochodnych cząstkowych funkcji kosztu względem parametrów modelu. Wskazuje kierunek, w którym funkcja kosztu rośnie najszybciej. Proces treningu polega na iteracyjnym aktualizowaniu parametrów modelu, aby zmniejszyć wartość funkcji kosztu. \n",
    "- Parametr zwany \"szybkością uczenia\" kontroluje, jak duże kroki są podejmowane podczas aktualizacji parametrów modelu. Zbyt duże kroki mogą prowadzić do rozbieżności, a zbyt małe kroki mogą spowodować wolny postęp. Optymalna wartość szybkości uczenia jest ważna.\n",
    "- Proces treningu polega na wielokrotnym obliczaniu gradientu, aktualizowaniu parametrów modelu i powtarzaniu tego procesu przez wiele iteracji. Ilość iteracji zależy od algorytmu i problemu, który rozwiązujemy.\n",
    "- Proces treningu kończy się, gdy osiągniemy satysfakcjonujący poziom zbieżności, czyli kiedy wartość funkcji kosztu jest już na niskim poziomie lub przestaje się znacząco zmieniać.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0d1a1735bd163b32102b8ea4514749acd69bcad8d489552073ec2e1a14a559f4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
